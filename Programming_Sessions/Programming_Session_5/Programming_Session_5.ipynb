{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A84sSMXE3hbZ"
      },
      "source": [
        "\n",
        "# **<center>Machine Learning and Finance </center>**\n",
        "\n",
        "\n",
        "## <center> Programming Sessing 5 - Solution - </center>\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"center\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://mlfbg.github.io/MachineLearningInFinance/\">\n",
        "    <img src=\"https://drive.google.com/uc?export=view&id=1gmxxmwCR1WXK0IYtNqvE4QXFleznWqQO\" height=\"50\"/>\n",
        "    Course page</a>\n",
        "</td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/drive/1OjRifPKpVqnT7ZFpfy0rNYaxWBDqpPUd?usp=sharing\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" height=\"50\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Presentation of the Programming Session:\n"
      ],
      "metadata": {
        "id": "i3UNZ1BIqRBM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNbuq3vZIg-Q"
      },
      "source": [
        "\n",
        "\n",
        "In this programming session, we would like to implement the GloVe approach. It was introduced by Jeffrey Pennington, Richard Socher and  Christopher D. Manning in the paper: [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)\n",
        "\n",
        "The programming session is subdivided into three parts:\n",
        "\n",
        "* In section 1, the objective is to load the data, preprocess it and create the **co-occurence matrix**. \n",
        "* In section 2, the objective is to train the model by using two different methods: **Gradient Descent** and **Alternating Least Squares**. \n",
        "* In section 3, the objective is to add a penalty term to the loss function as a **regularization** technique. \n",
        "\n",
        "**Notations:**\n",
        "\n",
        "* $\\mathcal{M}_{n,p}(\\mathbb{R})$ is the space of the matrices composed of n rows and p columns.\n",
        "\n",
        "* $I_n \\in \\mathcal{M}_{n,n}(\\mathbb{R})$ is the identity matrix of size n.\n",
        "\n",
        "* For all $z \\in \\mathbb{R}^D$, the $\\mathcal{L}^2$ norm on $\\mathbb{R}^D$ of $z$ is defined as follows: $||z||_2^2 = z^T z$\n",
        "\n",
        "*  For all $A = [a_{ij}]_{i,j} \\in \\mathcal{M}_{n,p}(\\mathbb{R})$ we define the Frobenius norm of $A$ as follows: $||A||_{\\text{F}}^2 = \\sum\\limits_{i=1}^n \\sum\\limits_{j=1}^p a_{ij}^2$ \n",
        "\n",
        "* The gradient of a function $f : \\theta \\in \\mathbb{R}^D \\mapsto \\mathbb{R}$ at $\\theta\\in \\mathbb{R}^D$ is denoted as follows $\\nabla_{\\theta}f(\\theta) = \\left(\\frac{\\partial f}{\\partial \\theta_1}(\\theta), \\dots, \\frac{\\partial f}{\\partial \\theta_D}(\\theta) \\right)$\n",
        "\n",
        "**Convention:** \n",
        "\t\n",
        "* The rows $(A_i)_{1 \\leq i \\leq n }$ of a matrix $A = \\begin{pmatrix}\n",
        "- & A_1 & - \\\\\n",
        "\\vdots & \\vdots & \\vdots \\\\\n",
        "- & A_n & - \n",
        "\\end{pmatrix}\\in \\mathcal{M}_{n,p}(\\mathbb{R}) $ are \t\t\t\tconsidered $\\mathcal{M}_{p,1}(\\mathbb{R})$ matrices. \n",
        "\n",
        "* The columns $(B_j)_{1 \\leq j \\leq p }$ of a matrix $B = \\begin{pmatrix}\n",
        "| & \\dots & | \\\\\n",
        "B_1 & \\dots & B_p \\\\\n",
        "| & \\dots & | \n",
        "\\end{pmatrix}\\in \\mathcal{M}_{n,p}(\\mathbb{R}) $ are considered $\\mathcal{M}_{n,1}(\\mathbb{R})$ matrices. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HS0eX7Kebja4",
        "outputId": "2656ba9a-7735-4397-8cc9-b1c321a62c5a"
      },
      "source": [
        "# Access files from Google Drive\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "os.chdir('./gdrive/My Drive/Teaching/Imperial_College/Colab Notebooks/Programming_Session_5/')"
      ],
      "metadata": {
        "id": "TXs9t3aPBEuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCcWOvm03Zxu"
      },
      "source": [
        "# Import basic libraries\n",
        "import pandas as pd # for dataframes\n",
        "import numpy as np # for arrays\n",
        "import matplotlib.pyplot as plt # for plots \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer # for processing text\n",
        "import random # to shuffle the sequences\n",
        "import os \n",
        "plt.style.use('dark_background') # to adapt the colors to a dark background\n",
        "from IPython.display import Image # for showing graphs from the lectures"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yDVjd7FbF9L"
      },
      "source": [
        "# Hyperparameters\n",
        "MAX_VOCAB = 999\n",
        "C = 10 # Context size\n",
        "V = MAX_VOCAB + 1 # Vocabulary size\n",
        "EPOCHS = 64\n",
        "D = 100 # Embedding dimension"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEg4IWrPY6QE"
      },
      "source": [
        "# 1. Getting the statistics of the word occurences "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_myLbIjnJQx4"
      },
      "source": [
        "## 1.1 Introducing the problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pf_Enu1KgPy"
      },
      "source": [
        "The objective of the programming session is to train a model on a corpus of training sentences in order to represent words in a $D$-dimensional space. We would like to encode the similarity between the words in the embedding vectors themselves. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SutTRTQJT4J"
      },
      "source": [
        "\n",
        "---\n",
        "<font color=green>Q1:</font>\n",
        "<br><font color='green'>\n",
        "Explain why this notion of similarity is not encoded in the one hot vector representation of words.\n",
        "</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDVPk01tOsfT"
      },
      "source": [
        "---\n",
        "**Solution**:\n",
        "\n",
        "Like explained in Slide 7 of [Lecture 5](https://hm-ai.github.io/MLF/Lectures/Lecture_5.pdf), any two V-dimensional one hot vectors will be orthogonal according to the dot product similarty measure.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmoXLYiyKlfr"
      },
      "source": [
        "Several methods have been used to create word embeddings. The most popular ones rely on the intuition that *a word's meaning is given by the words that frequently appear close-by*. \n",
        "\n",
        "For instance, [the word2vec approach](https://arxiv.org/pdf/1301.3781.pdf) represents the tokens as parameters of a shallow neural network predicting a word's context given the world itself. \n",
        "\n",
        "Although the shallow window-based model captures linguistic patterns between word vectors and performs well on the word analogy task ($w_{\\text{France}} - w_{\\text{Paris}} \\approx w_{\\text{England}} - w_{\\text{London}}$), the model suffers from the disadvantage that they do not operate directly and the co-occurence statistics.\n",
        "\n",
        "The GloVe method is a popular method used to learn low-dimensional word representations by using **matrix factorization** methods on a matrix of word-word **co-occurence** statistics. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qn9LBGBIj9K6"
      },
      "source": [
        "## 1.2 Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG-5uQsmTf9L"
      },
      "source": [
        "The **data** folder contains a csv file named `RedditNews.csv` (Source: Sun, J. (2016, August) Daily News for Stock Market Prediction, Version 1. Retrieved [26 may 2020]).\n",
        "\n",
        "In the `RedditNews.csv` file are stored historical news headlines from Reddit WorldNews Channel, ranked by reddit users' votes, and only the top 25 headlines are considered for a single date.\n",
        "\n",
        "You will find two colomns: \n",
        "\n",
        "\n",
        "* The first column is for the \"date\".\n",
        "* The second column is for the \"News\". As all the news are ranked from top to bottom, there are only 25 lines for each date.  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q2:</font>\n",
        "<br><font color='green'>\n",
        "Load the data from the csv file, create a list of all the news.\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "k6wCO8hAF3Vo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-JxcrF8Khhs"
      },
      "source": [
        "# Import the data\n",
        "\n",
        "# Select the news\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "IhGBiEn-BOTy",
        "outputId": "ae90fdfa-c1cd-46cb-9151-e700d87ec257"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             Date                                               News\n",
              "0      2016-07-01  A 117-year-old woman in Mexico City finally re...\n",
              "1      2016-07-01   IMF chief backs Athens as permanent Olympic host\n",
              "2      2016-07-01  The president of France says if Brexit won, so...\n",
              "3      2016-07-01  British Man Who Must Give Police 24 Hours' Not...\n",
              "4      2016-07-01  100+ Nobel laureates urge Greenpeace to stop o...\n",
              "...           ...                                                ...\n",
              "73603  2008-06-08  b'Man goes berzerk in Akihabara and stabs ever...\n",
              "73604  2008-06-08  b'Threat of world AIDS pandemic among heterose...\n",
              "73605  2008-06-08  b'Angst in Ankara: Turkey Steers into a Danger...\n",
              "73606  2008-06-08  b\"UK: Identity cards 'could be used to spy on ...\n",
              "73607  2008-06-08  b'Marriage, they said, was reduced to the stat...\n",
              "\n",
              "[73608 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-80635d02-1b46-40a1-b51f-ee32d8bfa385\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>News</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-07-01</td>\n",
              "      <td>A 117-year-old woman in Mexico City finally re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-07-01</td>\n",
              "      <td>IMF chief backs Athens as permanent Olympic host</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016-07-01</td>\n",
              "      <td>The president of France says if Brexit won, so...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016-07-01</td>\n",
              "      <td>British Man Who Must Give Police 24 Hours' Not...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2016-07-01</td>\n",
              "      <td>100+ Nobel laureates urge Greenpeace to stop o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73603</th>\n",
              "      <td>2008-06-08</td>\n",
              "      <td>b'Man goes berzerk in Akihabara and stabs ever...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73604</th>\n",
              "      <td>2008-06-08</td>\n",
              "      <td>b'Threat of world AIDS pandemic among heterose...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73605</th>\n",
              "      <td>2008-06-08</td>\n",
              "      <td>b'Angst in Ankara: Turkey Steers into a Danger...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73606</th>\n",
              "      <td>2008-06-08</td>\n",
              "      <td>b\"UK: Identity cards 'could be used to spy on ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73607</th>\n",
              "      <td>2008-06-08</td>\n",
              "      <td>b'Marriage, they said, was reduced to the stat...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>73608 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-80635d02-1b46-40a1-b51f-ee32d8bfa385')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-80635d02-1b46-40a1-b51f-ee32d8bfa385 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-80635d02-1b46-40a1-b51f-ee32d8bfa385');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSiWG1LOTH8-"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "<font color=green>Q3:</font>\n",
        "<br><font color='green'>\n",
        "Preprocess the data by transforming the list of sentences into a list of sequences of integers called `news_processed` , via a dictionary that maps the words to integers. \n",
        "</font>\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<center><img width=\"400\" src = \"https://drive.google.com/uc?export=view&id=15_ocFu9iG0sOPmK0dKhECzTUbIXFSeHC\"></center>"
      ],
      "metadata": {
        "id": "s5grXRA_CgxV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qACCC4u1LHau"
      },
      "source": [
        "# Preprocessing \n",
        "tokenizer = Tokenizer(num_words = MAX_VOCAB,\n",
        "                      oov_token='UNK',\n",
        "                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
        "                      lower=True)\n",
        "# Create the word_index dictionary\n",
        "\n",
        "# Transforming news into a list of lists of integers\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q4:</font>\n",
        "<br><font color='green'>\n",
        "For each sentence, add a specific index for the token \"$<\\text{sos}>$\" (start of sequence) at the beginning of each sequence and an index for the token \"$<\\text{eos}>$\" (end of sequence) at the end of each sequence. The resulting list of lists of integers is called `sequences`. \n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "gsuspkmgDvd3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UUGI0oNLWRk"
      },
      "source": [
        "# Introduce the tokens \"<sos>\" and \"<eos>\":\n",
        "word_index = {}\n",
        "word_index['<sos>'] = 0\n",
        "for k, v in tokenizer.word_index.items():\n",
        "    if v < MAX_VOCAB:\n",
        "        word_index[k] = v\n",
        "word_index['<eos>'] = MAX_VOCAB\n",
        "\n",
        "# Shuffle the sentences\n",
        "random.shuffle(news_processed)\n",
        "\n",
        "# add the indices of <sos> and <eos>\n",
        "sequences = []\n",
        "for sequence in news_processed:\n",
        "    sequences.append([0] + sequence + [MAX_VOCAB])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37bxb3akkE4B"
      },
      "source": [
        "## 1.3 Creating the co-occurence matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMXi6HniTk6j"
      },
      "source": [
        "Let $V$ be the vocabulary size of the training corpus. \n",
        "\n",
        "In [Lecture 5](https://hm-ai.github.io/MLF/Lectures/Lecture_5.pdf), we have defined the co-occurence matrix $X = [X_{ij}]_{i,j} \\in \\mathcal{M}_{V,V}(\\mathbb{R})$, whose entries $X_{ij}$ represent the number of times word $j$ appears in the context of word $i$. \n",
        "\n",
        "Algorithm 1 summarizes the steps involved in estimating the co-occurence matrix from the corpus `sequences`.\n",
        "\n",
        "<center><img width=\"400\" src = \"https://drive.google.com/uc?export=view&id=186c4b_X8mDEgBoVNZKEw7sfXOzKa-KN-\"></center>\n",
        "\n",
        "\n",
        "In Algorithm 1, each time a word $w[j]$ (of index $j$ in sequence) appears in the context of a center word $w[i]$ (of index $i$ in sequence), we increase the value of $X[w[i], w[j]]$ by a value of $1$ regardless of how close the word $w[j]$ is to the word $w[i]$. \n",
        "\n",
        "We would like to take into consideration the distance $d(i,j)$ between the center word $w[i]$ and the context word $w[j]$ when updating the value $X[w[i], w[j]]$, as shown the following figure:\n",
        "\n",
        "<center><img width=\"600\" src = \"https://drive.google.com/uc?export=view&id=1m1_32ovMfjkRVb-B_3gPizDI1QUfZjQ4\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q5:</font>\n",
        "<br><font color='green'>\n",
        "Explain why it makes more sense to use the following update equation for $X[w[i], w[j]]$ when word $w[j]$ of index $j$ is in the context word $w[i]$ of index $i$. \n",
        " \n",
        "\\begin{equation*}\n",
        "X[w[i], w[j]] \\longleftarrow X[w[i], w[j]] + \\frac{1}{|i-j|}\n",
        "\\end{equation*}\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "e1M45t0GD0xC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVnkACgmLcub"
      },
      "source": [
        "---\n",
        "**Solution**\n",
        "\n",
        "* By using the update: $X[w[i], w[j]] \\longleftarrow X[w[i], w[j]] + 1$ we make the assumption that all the words $w[j]$ are equally important as context vectors of center word $w[i]$\n",
        "\n",
        "* By using the update: $X[w[i], w[j]] \\longleftarrow X[w[i], w[j]] + \\frac{1}{|i-j|}$, we would like to give more weight to closer context words because they are more related to the center word. \n",
        "\n",
        "* Another benefit from the update $X[w[i], w[j]] \\longleftarrow X[w[i], w[j]] + \\frac{1}{|i-j|}$ is to reduce the impact of the `context_size` hyperparameter. \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q6:</font>\n",
        "<br><font color='green'>\n",
        "Implement Algorithm 2 to get the co-occurence matrix $X$ using a function called `get_cooccurence_matrix()`.\n",
        "\n",
        "The function takes as arguments `sequences`, `context_size` and `vocabulary_size` and outputs the matrix `X`.\n",
        "</font>\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<center><img width=\"400\" src = \"https://drive.google.com/uc?export=view&id=1Xt6SXVNxlsPHqIWk1IIaLOZcpTrQWZSd\"></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "XI-bKFPhD2Bq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hlpk4oBmHfNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcufa32cNtgM",
        "outputId": "05cf367f-ae58-4f82-f16a-9fada841a263"
      },
      "source": [
        "print(\"Get the co-occurence matrix X...\")\n",
        "X = get_occurence_matrix(sequences, C, V)\n",
        "print(\"The shape of X is\", X.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get the co-occurence matrix X...\n",
            "number of sentences to process: 73608\n",
            "processed 10000 / 73608\n",
            "processed 20000 / 73608\n",
            "processed 30000 / 73608\n",
            "processed 40000 / 73608\n",
            "processed 50000 / 73608\n",
            "processed 60000 / 73608\n",
            "processed 70000 / 73608\n",
            "The shape of X is (1000, 1000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zM372IuTZlr"
      },
      "source": [
        "\n",
        "\n",
        "Since non-zero values in the matrix $X$ are very large, we apply the logarithm function to all the elements of $X$ (after adding 1 to all the entries $X_{ij}$ to avoid applying the logarithm on zero values). The resulting matrix is still a sparse matrix. We will denote it $\\log X$.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q7:</font>\n",
        "<br><font color='green'>\n",
        "Create the matrix $\\log X$, call it `logX`.\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "U4c7UN5CD3ba"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8ANrmOAOEb8",
        "outputId": "f923aec0-0d45-4e6e-dfe5-8ad6b12bffc9"
      },
      "source": [
        "print(\"Get logX...\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get logX...\n",
            "The shape of logX is (1000, 1000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNTHnF-LkFFK"
      },
      "source": [
        "# 2. Training the weighted least squares regression model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHIrgh9bkFM-"
      },
      "source": [
        "## 2.1 Introducing the cost function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r59PLdpZkFXS"
      },
      "source": [
        "The logarithm of the co-occurence matrix $\\log X$ has been defined in the previous section. The objective of this section is to approximate $\\log X$ using a factorization method as follows:\n",
        "\n",
        "\\begin{equation*}\n",
        "\t\\forall (i,j) \\in \\{1, \\dots, V \\}^2 \\quad \\log X_{ij} \\approx W_i^T \\tilde{W}_j + b_i + \\tilde{b}_j\n",
        "\\end{equation*}\n",
        "\n",
        "\n",
        "The parameters of the regression model are:\n",
        "\n",
        "\n",
        "* A first **embedding matrix** and a bias term associated with it:\n",
        "\t\\begin{equation*}\n",
        "W = \\begin{pmatrix}\n",
        "- & W_1 & - \\\\\n",
        "\\vdots & \\vdots & \\vdots \\\\\n",
        "- & W_V & - \n",
        "\\end{pmatrix}\\in \\mathcal{M}_{V, D}(\\mathbb{R}), \t\\quad  \tb = \\begin{pmatrix}\n",
        " b_1  \\\\\n",
        " \\vdots  \\\\\n",
        " b_V  \n",
        "\\end{pmatrix}\\in \\mathbb{R}^{V}\n",
        "\t\\end{equation*}\t\t\n",
        "\t\n",
        "\n",
        "* A second **embedding matrix** and a bias term associated with it:\n",
        "\t\\begin{equation*}\n",
        "\\tilde{W} = \\begin{pmatrix}\n",
        "- & \\tilde{W}_1 & - \\\\\n",
        "\\vdots & \\vdots & \\vdots \\\\\n",
        "- & \\tilde{W}_V & - \n",
        "\\end{pmatrix}\\in \\mathcal{M}_{V, D}(\\mathbb{R}), \\quad \\tilde{b} = \\begin{pmatrix}\n",
        " \\tilde{b}_1  \\\\\n",
        " \\vdots  \\\\\n",
        " \\tilde{b}_V  \n",
        "\\end{pmatrix}\\in \\mathbb{R}^{V} \t\n",
        "\t\\end{equation*}\t\n",
        "\n",
        "\n",
        "Instead of equal-weighting all the co-occurences, we introduce a **weighting function** $f(X_{ij})$ defined as follows: \n",
        "\n",
        "\\begin{equation*}\n",
        "\\forall x \\in \\mathbb{R}_{+} \\quad f(x) =  \\begin{cases}\n",
        "      (x/x_{\\text{max}})^{\\alpha} & \\text{if   $x < x_{\\text{max}}$}\\\\\n",
        "      1 & \\text{otherwise}\n",
        "          \\end{cases}  \n",
        "\\end{equation*}\n",
        "\n",
        "\n",
        "The function $f$ is represented in the following figure with $x_{\\text{max}}=100$ and $\\alpha=0.75$\n",
        "\n",
        "<center><img width=\"400\" src = \"https://drive.google.com/uc?export=view&id=1D7muXkREj-5pPVUWyAfe8qrwYeIe0XzN\"></center>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q8:</font>\n",
        "<br><font color='green'>\n",
        "Create a matrix of shape $(V, V)$ whose entries are $f(X_{ij})$. \n",
        "Let's call it `fX`. Use the hyperparameters $x_{\\text{max}}=100$ and $\\alpha=0.75$\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "dBlWZ8ZsD4qx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEnJbfO6ObLg",
        "outputId": "abf37a23-3f82-4d32-8c5f-d1efc2eba325"
      },
      "source": [],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get f(X)...\n",
            "The shape of fX is (1000, 1000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q9:</font>\n",
        "<br><font color='green'>\n",
        "What are the hyperparameters associated with the weighting function and what is the intuition behind introducing it?\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "1ofPy91eD5UN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWbvjvDsO8Cy"
      },
      "source": [
        "---\n",
        "**Solution:**\n",
        "* There are two hyperparameters associated with the weighting function: $x_{\\text{max}}$ and $\\alpha$\n",
        "\n",
        "* By introducing the weighting function, we make the assumption that rare occurences are noisy and carry less information than the more frequent ones. Therefore, when the entry $X_{ij}$ is small, we would like to reduce the contribution of the loss term associated with it (i.e, $(\\log X_{ij} - W_i^T \\tilde{W}_j - b_i - \\tilde{b}_j)^2$) to the global loss. \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjWn9PfcTzqW"
      },
      "source": [
        "The **cost function** can then be written as follows: \n",
        "\n",
        "\\begin{equation*}\n",
        "J = \\sum_{i=1}^V \\sum_{j=1}^V f(X_{ij}) (\\log X_{ij} - W_i^T \\tilde{W}_j - b_i - \\tilde{b}_j)^2\n",
        "\\end{equation*}\n",
        "\n",
        "\n",
        "The gradients of the cost function $J$ with respect to all the parameters are introduced in the following equations:\n",
        "\n",
        "\n",
        "For all $i \\in \\{1, \\dots, V \\}$ and all $j \\in \\{ 1, \\dots, V \\}$:\n",
        "\n",
        "\n",
        "\\begin{align} \n",
        "& \\nabla_{W_i} J(W_i) = -2 \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - b_i - \\tilde{b}_{j'} \\right) \\tilde{W}_{j'} \\quad \\text{(2.1)} \\\\\n",
        "& \\nabla_{\\tilde{W}_j} J(W_j) = -2 \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'} - \\tilde{b}_j \\right) W_{i'} \\quad \\text{(2.2)}  \\\\\n",
        "&\\nabla_{b_i} J(b_i) = -2 \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - b_i - \\tilde{b}_{j'} \\right) \\quad \\text{(2.3)}  \\\\\n",
        "& \\nabla_{\\tilde{b}_j} J(\\tilde{b}_j) = -2 \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'} - \\tilde{b}_j \\right) \\quad \\text{(2.4)} \n",
        "\\end{align}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q10:</font>\n",
        "<br><font color='green'>\n",
        "What is the total number of parameters in the model ? What are the shapes of all the gradients introduced in the equations (2.1), (2.2), (2.3) and (2.4) ?\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "XbzQWdyLD6Q2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDaCk9RMdYAr"
      },
      "source": [
        "---\n",
        "**Solution:**\n",
        "\n",
        "* Number of parameters:\n",
        "  * The matrices $W$ and $\\tilde{W}$ are both in $\\mathcal{M}_{V, D}(\\mathbb{R})$.\n",
        "  * The vectors $b$ and $\\tilde{b}$ are both $V$-dimensional vectors.\n",
        "  * So the total number of parameters $N_{\\text{parameters}}$ is \n",
        "\n",
        "  \\begin{equation}\n",
        "    N_{\\text{parameters}} = 2 (V D) + 2 V\n",
        "  \\end{equation}\n",
        "\n",
        "* Shapes of gradients:\n",
        "\n",
        "  * For all $i \\in \\{1, \\dots, V \\}$ and for all $j \\in \\{1, \\dots, V \\}$:\n",
        "\n",
        "\\begin{align} \n",
        "& \\nabla_{W_i} J(W_i) \\in \\mathcal{M}_{D, 1}(\\mathbb{R}) \\\\\n",
        "& \\nabla_{\\tilde{W}_j} J(W_j) \\in \\mathcal{M}_{D, 1}(\\mathbb{R})  \\\\\n",
        "&\\nabla_{b_i} J(b_i)  \\in \\mathbb{R}  \\\\\n",
        "& \\nabla_{\\tilde{b}_j} J(\\tilde{b}_j) \\in \\mathbb{R}\n",
        "\\end{align}\n",
        "    \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rZEuCrPT3cH"
      },
      "source": [
        "Let us introduce two training methods:\n",
        "\n",
        "* The first training method is called **alternating least squares**. It consists in finding the update equations by setting all the gradients to zero.\n",
        "* The second training method consists in applying the **gradient descent** algorithm.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbPZ_vBQkFdg"
      },
      "source": [
        "## 2.2 Alternating least squares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-i1DbdJkFib"
      },
      "source": [
        "We would like to estimate the parameters $W, \\tilde{W}, b, \\tilde{b}$ by setting the gradients to zero. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "By setting the gradients to zero, we get the following update equations: \n",
        "\n",
        "\\begin{align*}\n",
        "&\\nabla_{W_i} J(W_i) = 0 \\iff W_i = \\left( \\sum_{j'=1}^V f(X_{ij'}) \\tilde{W}_{j'} \\tilde{W}_{j'}^T \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - b_i - \\tilde{b}_{j'}) \\tilde{W}_{j'} \\right)  \\\\\n",
        "&\\nabla_{\\tilde{W}_j} J(\\tilde{W}_j) = 0 \\iff \\tilde{W}_j = \\left( \\sum_{i'=1}^V f(X_{i' j}) W_{i'} W_{i'}^T \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - b_{i'} - \\tilde{b}_{j}) W_{i'} \\right)  \\\\\n",
        "&\\nabla_{b_i} J(b_i) = 0 \\iff b_i = \\left( \\sum_{j'=1}^V f(X_{ij'})  \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - \\tilde{b}_{j'}) \\right)  \\\\\n",
        "&\\nabla_{\\tilde{b}_j} J(\\tilde{b}_j) = 0 \\iff \\tilde{b}_j = \\left( \\sum_{i'=1}^V f(X_{i' j})  \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - W_{i'}^T \\tilde{W}_{j} - b_{i'}) \\right) \n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "The proof can be found in **appendix A**"
      ],
      "metadata": {
        "id": "InI0dBGgD7tN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO0xwC4eXwAp"
      },
      "source": [
        "Each update equation for one parameter is a function of the other parameters. Therefore, in order to train our model, we can choose a number of iterations $N_{\\text{epochs}}$, and apply the update equations $N_{\\text{epochs}}$ times by keeping track of the loss to make sure it converges. \n",
        "\n",
        "For each iteration step $t \\in \\{0, \\dots, N_{\\text{epochs}}-1 \\}$, let $W^{(t)}, \\tilde{W}^{(t)}, b^{(t)}, \\tilde{b}^{(t)}$ represent the parameters of our model at the iteration $t$. \n",
        "\n",
        "\n",
        "The update equations from iteration $t$ to $t+1$ can then be written as follows:\n",
        "\n",
        "\\begin{align}\n",
        "&W_i^{(t+1)} \\longleftarrow \\left( \\sum_{j'=1}^V f(X_{ij'}) \\tilde{W}_{j'}^{(t)} \\tilde{W}_{j'}^{(t)^T} \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - b_i^{(t)} - \\tilde{b}_{j'}^{(t)}) \\tilde{W}_{j'}^{(t)} \\right) \\quad \\text{(2.5)} \\\\\n",
        "&\\tilde{W}_j^{(t+1)} \\longleftarrow \\left( \\sum_{i'=1}^V f(X_{i' j}) W_{i'}^{(t)} W_{i'}^{(t)^T} \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - b_{i'}^{(t)} - \\tilde{b}_{j}^{(t)}) W_{i'}^{(t)} \\right) \\quad \\text{(2.6)}  \\\\\n",
        "&b_i^{(t+1)} \\longleftarrow \\left( \\sum_{j'=1}^V f(X_{ij'})  \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - W_i^{(t)^T} \\tilde{W}_{j'}^{(t)} - \\tilde{b}_{j'}^{(t)}) \\right) \\quad \\text{(2.7)} \\\\\n",
        "&\\tilde{b}_j^{(t+1)} \\longleftarrow \\left( \\sum_{i'=1}^V f(X_{i' j})  \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - W_{i'}^{(t)^T} \\tilde{W}_{j}^{(t)} - b_{i'}^{(t)}) \\right) \\quad \\text{(2.8)}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\n",
        "The pseudo code for the training algorithm can be expressed as follows: \n",
        "\n",
        "\n",
        "<center><img width=\"500\" src = \"https://drive.google.com/uc?export=view&id=1DQmP3N13RH2hAP2-Szgk8TPhzcHICwGU\"></center>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q11:</font>\n",
        "<br><font color='green'>\n",
        "Implement the alternating least squares training algorithm\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "my9j51piD8-d"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "62nQJ9gmaNk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q12:</font>\n",
        "<br><font color='green'>\n",
        " Plot the list of losses at the end of each iteration in Algorithm 3.\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0cmFRYw-D95S"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L7br03pNbwRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX56bVMHkFnd"
      },
      "source": [
        "## 2.3 Learning the weights using gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Luje3fIdkFsY"
      },
      "source": [
        "In this section, we would like to estimate the parameters of the model using gradient descent.\n",
        "\n",
        "Let $N_{\\text{epochs}}$ be the number of epochs and $\\eta$ be the learning rate. \n",
        "We get the following training algorithm:\n",
        "\n",
        "\n",
        "<center><img width=\"500\" src = \"https://drive.google.com/uc?export=view&id=1Od3xCvMWKOBhMpccmKOtoY3aT3UTJB5Z\"></center>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q13:</font>\n",
        "<br><font color='green'>\n",
        "Implement the gradient descent training algorithm (Algorithm 4).\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "85Ih7k7OD-py"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xY4e1IP3mKL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q14:</font>\n",
        "<br><font color='green'>\n",
        " Plot the list of losses at the end of each iteration in Algorithm 4.\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "6aS7uO42D_Ya"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VRIWLWrmfnR"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd9PcTFwkFwo"
      },
      "source": [
        "# 3. Exercise: Introducing regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWkYPM1dkF1u"
      },
      "source": [
        "Let us introduce a regularization penalty term in the cost function. The new cost function is defined as follows:\n",
        "\n",
        "\n",
        "\\begin{equation*}\n",
        "\\tilde{J} = \\sum\\limits_{i=1}^V \\sum\\limits_{j=1}^V f(X_{ij}) (\\log X_{ij} - W_i^T \\tilde{W}_j - b_i - \\tilde{b}_j)^2 + \\lambda \\left( ||W||_{\\text{F}}^2 +   ||\\tilde{W}||_{\\text{F}}^2 + ||b||_2^2 + ||\\tilde{b} ||_2^2 \\right) \n",
        "\\end{equation*}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q15:</font>\n",
        "<br><font color='green'>\n",
        "Show that: \n",
        "\\begin{equation}\n",
        "||W||_{\\text{F}}^2 = \\sum\\limits_{i=1}^V W_i^T W_i\n",
        "\\end{equation}\n",
        "\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0Fgcb-ASEAad"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT6sJx8wTDaE"
      },
      "source": [
        "---\n",
        "**Write your answer:**\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q16:</font>\n",
        "<br><font color='green'>\n",
        "Deduce that for all $i \\in \\{1, \\dots, V \\}$:\n",
        "\\begin{align} \n",
        "& \\nabla_{W_i} (||W||_{\\text{F}}^2) = 2W_i \\quad \\text{(3.1)} \\\\ \n",
        "&(\\text{Hint}: \\forall z \\in \\mathbb{R}^D \\ \\forall A \\in \\mathcal{M}_{D, D}(\\mathbb{R}) \\quad \\nabla_z (z^T A z) = (A + A^T)z )\n",
        "\\end{align} \n",
        "\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "JgdEJEcFEBCh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Wcp4if-VP9y"
      },
      "source": [
        "---\n",
        "**Write your answer here**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q17:</font>\n",
        "<br><font color='green'>\n",
        "From the equations(2.1), (2.2), (2.3), (2.4) and (3.1), show that the update equations for the method of alternating least squares become: \n",
        "\n",
        "\\begin{align*}\n",
        "&W_i^{(t+1)} \\longleftarrow \\left( \\sum_{j'=1}^V f(X_{ij'}) \\tilde{W}_{j'}^{(t)} \\tilde{W}_{j'}^{(t)^T} + \\lambda I_D \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - b_i^{(t)} - \\tilde{b}_{j'}^{(t)}) \\tilde{W}_{j'}^{(t)} \\right)  \\\\\n",
        "&\\tilde{W}_j^{(t+1)} \\longleftarrow \\left( \\sum_{i'=1}^V f(X_{i' j}) W_{i'}^{(t)} W_{i'}^{(t)^T} + \\lambda I_D \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - b_{i'}^{(t)} - \\tilde{b}_{j}^{(t)}) W_{i'}^{(t)} \\right) \\\\\n",
        "&b_i^{(t+1)} \\longleftarrow \\left( \\sum_{j'=1}^V f(X_{ij'}) + \\lambda  \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - W_i^{(t)^T} \\tilde{W}_{j'}^{(t)} - \\tilde{b}_{j'}^{(t)}) \\right)  \\\\\n",
        "&\\tilde{b}_j^{(t+1)} \\longleftarrow \\left( \\sum_{i'=1}^V f(X_{i' j}) + \\lambda  \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - W_{i'}^{(t)^T} \\tilde{W}_{j}^{(t)} - b_{i'}^{(t)}) \\right) \n",
        "\\end{align*}\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "HcfbBXP2EBpp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTCLv_fKcEej"
      },
      "source": [
        "---\n",
        "**Write your answoer here:**\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q18:</font>\n",
        "<br><font color='green'>\n",
        " What would be the update equations for minimizing the new loss function $\\tilde{J}$ by using the gradient descent algorithm.\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "OcsjfGD9EC2C"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-7Z2AL-hU2w"
      },
      "source": [
        "---\n",
        "**Write your answer here:**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix "
      ],
      "metadata": {
        "id": "I8J7ErjCVRJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<br><font color='green'>\n",
        "Let us show that:\n",
        "\n",
        "\\begin{align*}\n",
        "&\\nabla_{W_i} J(W_i) = 0 \\iff W_i = \\left( \\sum_{j'=1}^V f(X_{ij'}) \\tilde{W}_{j'} \\tilde{W}_{j'}^T \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - b_i - \\tilde{b}_{j'}) \\tilde{W}_{j'} \\right)  \\\\\n",
        "&\\nabla_{\\tilde{W}_j} J(\\tilde{W}_j) = 0 \\iff \\tilde{W}_j = \\left( \\sum_{i'=1}^V f(X_{i' j}) W_{i'} W_{i'}^T \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - b_{i'} - \\tilde{b}_{j}) W_{i'} \\right)  \\\\\n",
        "&\\nabla_{b_i} J(b_i) = 0 \\iff b_i = \\left( \\sum_{j'=1}^V f(X_{ij'})  \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - \\tilde{b}_{j'}) \\right)  \\\\\n",
        "&\\nabla_{\\tilde{b}_j} J(\\tilde{b}_j) = 0 \\iff \\tilde{b}_j = \\left( \\sum_{i'=1}^V f(X_{i' j})  \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - W_{i'}^T \\tilde{W}_{j} - b_{i'}) \\right) \n",
        "\\end{align*}\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ZxPxjvefVel3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--- \n",
        "**Proof:**\n",
        "\n",
        "First, we need to prove a preliminary result:\n",
        "\n",
        "\\begin{equation*}\n",
        "  \\forall a,b \\in \\mathcal{M}_{D, 1}(\\mathbb{R}) \\quad (a^T b) \\ b = (b \\ b^T) \\ a \\quad (\\Delta)\n",
        "\\end{equation*}\n",
        "\n",
        "\n",
        "Indeed, \n",
        "\n",
        "\\begin{align}\n",
        "\\forall a,b \\in \\mathcal{M}_{D, 1}(\\mathbb{R}) \\quad (a^T b) \\ b  &= b \\ (a^T b) \\\\\n",
        "&= b \\ (b^T a) \\quad (\\text{As} \\ a^Tb \\ \\text{is a scalar, it's equal to its transpose}) \\\\\n",
        "&=  (b b^T) \\ a \n",
        "\\end{align}\n",
        "\n",
        "For all $i \\in \\{1, \\dots, V \\}$ and for all $j \\in \\{1, \\dots, V \\}$. \n",
        "\n",
        "Let us find the optimal parameters by setting the gradients to 0:\n",
        "\n",
        "* For $W_i$:\n",
        "\\begin{align*}\n",
        "\\nabla_{W_i} J(W_i) = 0 & \\iff -2 \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - b_i - \\tilde{b}_{j'} \\right) \\tilde{W}_{j'} = 0 \\quad \\text{(From (2.1))} \\\\\n",
        "& \\iff \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - b_i - \\tilde{b}_{j'} \\right) \\tilde{W}_{j'} = \\sum_{j'=1}^V f(X_{ij'})  W_i^T \\tilde{W}_{j'} \\tilde{W}_{j'} \\\\\n",
        "& \\iff \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - b_i - \\tilde{b}_{j'} \\right) \\tilde{W}_{j'} = \\left( \\sum_{j'=1}^V f(X_{ij'})  \\tilde{W}_{j'} \\tilde{W}_{j'}^T \\right) W_i  \\quad (\\text{From} \\ (\\Delta)) \\\\\n",
        "& \\iff W_i = \\left( \\sum_{j'=1}^V f(X_{ij'}) \\tilde{W}_{j'} \\tilde{W}_{j'}^T \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - b_i - \\tilde{b}_{j'}) \\tilde{W}_{j'} \\right) \n",
        "\\end{align*}\n",
        "\n",
        "* For $\\tilde{W}_{j}$:\n",
        "\n",
        "\\begin{align*}\n",
        "\\nabla_{\\tilde{W}_j} J(\\tilde{W}_j) = 0 & \\iff  -2 \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'} - \\tilde{b}_j \\right) = 0  \\quad \\text{(From (2.2))} \\\\\n",
        "&\\iff \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - b_{i'} - \\tilde{b}_j \\right) W_{i'} = \\sum_{i'=1}^V f(X_{i' j})  W_{i'}^T \\tilde{W}_j  W_{i'} \\\\\n",
        "& \\iff \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - b_{i'} - \\tilde{b}_j \\right) W_{i'} = \\left( \\sum_{i'=1}^V f(X_{i' j})  W_{i'} W_{i'}^T \\right)  \\tilde{W}_j    \\\\\n",
        "& \\iff\\tilde{W}_j = \\left( \\sum_{i'=1}^V f(X_{i' j}) W_{i'} W_{i'}^T \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - b_{i'} - \\tilde{b}_{j}) W_{i'} \\right)\n",
        "\\end{align*}\n",
        "\n",
        "* For $b_i$:\n",
        "\\begin{align*}\n",
        "\\nabla_{b_i} J(b_i) = 0  & \\iff -2 \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - b_i - \\tilde{b}_{j'} \\right) = 0 \\quad \\text{(From (2.3))} \\\\\n",
        "& \\iff \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - \\tilde{b}_{j'} \\right)  = \\left( \\sum_{j'=1}^V f(X_{ij'}) \\right) b_i \\\\\n",
        "& \\iff b_i = \\left( \\sum_{j'=1}^V f(X_{ij'})  \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - \\tilde{b}_{j'}) \\right)\n",
        "\\end{align*}\n",
        "\n",
        "* For $\\tilde{b}_j$:\n",
        "\\begin{align*}\n",
        "\\nabla_{\\tilde{b}_j} J(\\tilde{b}_j) = 0  & \\iff -2 \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'} - \\tilde{b}_j \\right) = 0 \\quad \\text{(From (2.4))} \\\\\n",
        "& \\iff \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'}   \\right) = \\left(\\sum_{i'=1}^V f(X_{i' j}) \\right) \\tilde{b}_j \\\\\n",
        "& \\iff \\tilde{b}_j = \\left( \\sum_{i'=1}^V f(X_{i' j})  \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - W_{i'}^T \\tilde{W}_{j} - b_{i'}) \\right) \n",
        "\\end{align*}\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Fs8YgSFKXIHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Contact\n",
        "\n",
        "If you have any questions regarding this notebook, do not hesitate to contact: h.madmoun@imperial.ac.uk"
      ],
      "metadata": {
        "id": "r067lGnrw6t5"
      }
    }
  ]
}