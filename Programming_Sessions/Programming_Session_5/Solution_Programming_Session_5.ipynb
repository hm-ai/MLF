{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A84sSMXE3hbZ"
      },
      "source": [
        "\n",
        "# **<center>Machine Learning and Finance </center>**\n",
        "\n",
        "\n",
        "## <center> Programming Sessing 5 - Solution - </center>\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"center\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://mlfbg.github.io/MachineLearningInFinance/\">\n",
        "    <img src=\"https://drive.google.com/uc?export=view&id=1gmxxmwCR1WXK0IYtNqvE4QXFleznWqQO\" height=\"50\"/>\n",
        "    Course page</a>\n",
        "</td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/drive/1OjRifPKpVqnT7ZFpfy0rNYaxWBDqpPUd?usp=sharing\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" height=\"50\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Presentation of the Programming Session:\n"
      ],
      "metadata": {
        "id": "i3UNZ1BIqRBM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNbuq3vZIg-Q"
      },
      "source": [
        "\n",
        "\n",
        "In this programming session, we would like to implement the GloVe approach. It was introduced by Jeffrey Pennington, Richard Socher and  Christopher D. Manning in the paper: [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)\n",
        "\n",
        "The programming session is subdivided into three parts:\n",
        "\n",
        "* In section 1, the objective is to load the data, preprocess it and create the **co-occurence matrix**. \n",
        "* In section 2, the objective is to train the model by using two different methods: **Gradient Descent** and **Alternating Least Squares**. \n",
        "* In section 3, the objective is to add a penalty term to the loss function as a **regularization** technique. \n",
        "\n",
        "**Notations:**\n",
        "\n",
        "* $\\mathcal{M}_{n,p}(\\mathbb{R})$ is the space of the matrices composed of n rows and p columns.\n",
        "\n",
        "* $I_n \\in \\mathcal{M}_{n,n}(\\mathbb{R})$ is the identity matrix of size n.\n",
        "\n",
        "* For all $z \\in \\mathbb{R}^D$, the $\\mathcal{L}^2$ norm on $\\mathbb{R}^D$ of $z$ is defined as follows: $||z||_2^2 = z^T z$\n",
        "\n",
        "*  For all $A = [a_{ij}]_{i,j} \\in \\mathcal{M}_{n,p}(\\mathbb{R})$ we define the Frobenius norm of $A$ as follows: $||A||_{\\text{F}}^2 = \\sum\\limits_{i=1}^n \\sum\\limits_{j=1}^p a_{ij}^2$ \n",
        "\n",
        "* The gradient of a function $f : \\theta \\in \\mathbb{R}^D \\mapsto \\mathbb{R}$ at $\\theta\\in \\mathbb{R}^D$ is denoted as follows $\\nabla_{\\theta}f(\\theta) = \\left(\\frac{\\partial f}{\\partial \\theta_1}(\\theta), \\dots, \\frac{\\partial f}{\\partial \\theta_D}(\\theta) \\right)$\n",
        "\n",
        "**Convention:** \n",
        "\t\n",
        "* The rows $(A_i)_{1 \\leq i \\leq n }$ of a matrix $A = \\begin{pmatrix}\n",
        "- & A_1 & - \\\\\n",
        "\\vdots & \\vdots & \\vdots \\\\\n",
        "- & A_n & - \n",
        "\\end{pmatrix}\\in \\mathcal{M}_{n,p}(\\mathbb{R}) $ are \t\t\t\tconsidered $\\mathcal{M}_{p,1}(\\mathbb{R})$ matrices. \n",
        "\n",
        "* The columns $(B_j)_{1 \\leq j \\leq p }$ of a matrix $B = \\begin{pmatrix}\n",
        "| & \\dots & | \\\\\n",
        "B_1 & \\dots & B_p \\\\\n",
        "| & \\dots & | \n",
        "\\end{pmatrix}\\in \\mathcal{M}_{n,p}(\\mathbb{R}) $ are considered $\\mathcal{M}_{n,1}(\\mathbb{R})$ matrices. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HS0eX7Kebja4",
        "outputId": "2656ba9a-7735-4397-8cc9-b1c321a62c5a"
      },
      "source": [
        "# Access files from Google Drive\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "os.chdir('./gdrive/My Drive/Teaching/Imperial_College/Colab Notebooks/Programming_Session_5/')"
      ],
      "metadata": {
        "id": "TXs9t3aPBEuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCcWOvm03Zxu"
      },
      "source": [
        "# Import basic libraries\n",
        "import pandas as pd # for dataframes\n",
        "import numpy as np # for arrays\n",
        "import matplotlib.pyplot as plt # for plots \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer # for processing text\n",
        "import random # to shuffle the sequences\n",
        "import os \n",
        "plt.style.use('dark_background') # to adapt the colors to a dark background\n",
        "from IPython.display import Image # for showing graphs from the lectures"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yDVjd7FbF9L"
      },
      "source": [
        "# Hyperparameters\n",
        "MAX_VOCAB = 999\n",
        "C = 10 # Context size\n",
        "V = MAX_VOCAB + 1 # Vocabulary size\n",
        "EPOCHS = 64\n",
        "D = 100 # Embedding dimension"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEg4IWrPY6QE"
      },
      "source": [
        "# 1. Getting the statistics of the word occurences "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_myLbIjnJQx4"
      },
      "source": [
        "## 1.1 Introducing the problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pf_Enu1KgPy"
      },
      "source": [
        "The objective of the programming session is to train a model on a corpus of training sentences in order to represent words in a $D$-dimensional space. We would like to encode the similarity between the words in the embedding vectors themselves. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SutTRTQJT4J"
      },
      "source": [
        "\n",
        "---\n",
        "<font color=green>Q1:</font>\n",
        "<br><font color='green'>\n",
        "Explain why this notion of similarity is not encoded in the one hot vector representation of words.\n",
        "</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDVPk01tOsfT"
      },
      "source": [
        "---\n",
        "**Solution**:\n",
        "\n",
        "Like explained in Slide 7 of [Lecture 5](https://hm-ai.github.io/MLF/Lectures/Lecture_5.pdf), any two V-dimensional one hot vectors will be orthogonal according to the dot product similarty measure.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmoXLYiyKlfr"
      },
      "source": [
        "Several methods have been used to create word embeddings. The most popular ones rely on the intuition that *a word's meaning is given by the words that frequently appear close-by*. \n",
        "\n",
        "For instance, [the word2vec approach](https://arxiv.org/pdf/1301.3781.pdf) represents the tokens as parameters of a shallow neural network predicting a word's context given the world itself. \n",
        "\n",
        "Although the shallow window-based model captures linguistic patterns between word vectors and performs well on the word analogy task ($w_{\\text{France}} - w_{\\text{Paris}} \\approx w_{\\text{England}} - w_{\\text{London}}$), the model suffers from the disadvantage that they do not operate directly and the co-occurence statistics.\n",
        "\n",
        "The GloVe method is a popular method used to learn low-dimensional word representations by using **matrix factorization** methods on a matrix of word-word **co-occurence** statistics. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qn9LBGBIj9K6"
      },
      "source": [
        "## 1.2 Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG-5uQsmTf9L"
      },
      "source": [
        "The **data** folder contains a csv file named `RedditNews.csv` (Source: Sun, J. (2016, August) Daily News for Stock Market Prediction, Version 1. Retrieved [26 may 2020]).\n",
        "\n",
        "In the `RedditNews.csv` file are stored historical news headlines from Reddit WorldNews Channel, ranked by reddit users' votes, and only the top 25 headlines are considered for a single date.\n",
        "\n",
        "You will find two colomns: \n",
        "\n",
        "\n",
        "* The first column is for the \"date\".\n",
        "* The second column is for the \"News\". As all the news are ranked from top to bottom, there are only 25 lines for each date.  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q2:</font>\n",
        "<br><font color='green'>\n",
        "Load the data from the csv file, create a list of all the news.\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "k6wCO8hAF3Vo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-JxcrF8Khhs"
      },
      "source": [
        "# Import the data\n",
        "data = pd.read_csv(\"data/RedditNews.csv\")\n",
        "# Select the news\n",
        "news = data[\"News\"].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "IhGBiEn-BOTy",
        "outputId": "ae90fdfa-c1cd-46cb-9151-e700d87ec257"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             Date                                               News\n",
              "0      2016-07-01  A 117-year-old woman in Mexico City finally re...\n",
              "1      2016-07-01   IMF chief backs Athens as permanent Olympic host\n",
              "2      2016-07-01  The president of France says if Brexit won, so...\n",
              "3      2016-07-01  British Man Who Must Give Police 24 Hours' Not...\n",
              "4      2016-07-01  100+ Nobel laureates urge Greenpeace to stop o...\n",
              "...           ...                                                ...\n",
              "73603  2008-06-08  b'Man goes berzerk in Akihabara and stabs ever...\n",
              "73604  2008-06-08  b'Threat of world AIDS pandemic among heterose...\n",
              "73605  2008-06-08  b'Angst in Ankara: Turkey Steers into a Danger...\n",
              "73606  2008-06-08  b\"UK: Identity cards 'could be used to spy on ...\n",
              "73607  2008-06-08  b'Marriage, they said, was reduced to the stat...\n",
              "\n",
              "[73608 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-80635d02-1b46-40a1-b51f-ee32d8bfa385\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>News</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-07-01</td>\n",
              "      <td>A 117-year-old woman in Mexico City finally re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-07-01</td>\n",
              "      <td>IMF chief backs Athens as permanent Olympic host</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016-07-01</td>\n",
              "      <td>The president of France says if Brexit won, so...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016-07-01</td>\n",
              "      <td>British Man Who Must Give Police 24 Hours' Not...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2016-07-01</td>\n",
              "      <td>100+ Nobel laureates urge Greenpeace to stop o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73603</th>\n",
              "      <td>2008-06-08</td>\n",
              "      <td>b'Man goes berzerk in Akihabara and stabs ever...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73604</th>\n",
              "      <td>2008-06-08</td>\n",
              "      <td>b'Threat of world AIDS pandemic among heterose...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73605</th>\n",
              "      <td>2008-06-08</td>\n",
              "      <td>b'Angst in Ankara: Turkey Steers into a Danger...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73606</th>\n",
              "      <td>2008-06-08</td>\n",
              "      <td>b\"UK: Identity cards 'could be used to spy on ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73607</th>\n",
              "      <td>2008-06-08</td>\n",
              "      <td>b'Marriage, they said, was reduced to the stat...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>73608 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-80635d02-1b46-40a1-b51f-ee32d8bfa385')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-80635d02-1b46-40a1-b51f-ee32d8bfa385 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-80635d02-1b46-40a1-b51f-ee32d8bfa385');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSiWG1LOTH8-"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "<font color=green>Q3:</font>\n",
        "<br><font color='green'>\n",
        "Preprocess the data by transforming the list of sentences into a list of sequences of integers called `news_processed` , via a dictionary that maps the words to integers. \n",
        "</font>\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<center><img width=\"400\" src = \"https://drive.google.com/uc?export=view&id=15_ocFu9iG0sOPmK0dKhECzTUbIXFSeHC\"></center>"
      ],
      "metadata": {
        "id": "s5grXRA_CgxV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qACCC4u1LHau"
      },
      "source": [
        "# Preprocessing \n",
        "tokenizer = Tokenizer(num_words = MAX_VOCAB,\n",
        "                      oov_token='UNK',\n",
        "                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
        "                      lower=True)\n",
        "# Create the word_index dictionary\n",
        "tokenizer.fit_on_texts(news)\n",
        "# Transforming news into a list of lists of integers\n",
        "news_processed = tokenizer.texts_to_sequences(news)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q4:</font>\n",
        "<br><font color='green'>\n",
        "For each sentence, add a specific index for the token \"$<\\text{sos}>$\" (start of sequence) at the beginning of each sequence and an index for the token \"$<\\text{eos}>$\" (end of sequence) at the end of each sequence. The resulting list of lists of integers is called `sequences`. \n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "gsuspkmgDvd3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UUGI0oNLWRk"
      },
      "source": [
        "# Introduce the tokens \"<sos>\" and \"<eos>\":\n",
        "word_index = {}\n",
        "word_index['<sos>'] = 0\n",
        "for k, v in tokenizer.word_index.items():\n",
        "    if v < MAX_VOCAB:\n",
        "        word_index[k] = v\n",
        "word_index['<eos>'] = MAX_VOCAB\n",
        "\n",
        "# Shuffle the sentences\n",
        "random.shuffle(news_processed)\n",
        "\n",
        "# add the indices of <sos> and <eos>\n",
        "sequences = []\n",
        "for sequence in news_processed:\n",
        "    sequences.append([0] + sequence + [MAX_VOCAB])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37bxb3akkE4B"
      },
      "source": [
        "## 1.3 Creating the co-occurence matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMXi6HniTk6j"
      },
      "source": [
        "Let $V$ be the vocabulary size of the training corpus. \n",
        "\n",
        "In [Lecture 5](https://hm-ai.github.io/MLF/Lectures/Lecture_5.pdf), we have defined the co-occurence matrix $X = [X_{ij}]_{i,j} \\in \\mathcal{M}_{V,V}(\\mathbb{R})$, whose entries $X_{ij}$ represent the number of times word $j$ appears in the context of word $i$. \n",
        "\n",
        "Algorithm 1 summarizes the steps involved in estimating the co-occurence matrix from the corpus `sequences`.\n",
        "\n",
        "<center><img width=\"400\" src = \"https://drive.google.com/uc?export=view&id=186c4b_X8mDEgBoVNZKEw7sfXOzKa-KN-\"></center>\n",
        "\n",
        "\n",
        "In Algorithm 1, each time a word $w[j]$ (of index $j$ in sequence) appears in the context of a center word $w[i]$ (of index $i$ in sequence), we increase the value of $X[w[i], w[j]]$ by a value of $1$ regardless of how close the word $w[j]$ is to the word $w[i]$. \n",
        "\n",
        "We would like to take into consideration the distance $d(i,j)$ between the center word $w[i]$ and the context word $w[j]$ when updating the value $X[w[i], w[j]]$, as shown the following figure:\n",
        "\n",
        "<center><img width=\"600\" src = \"https://drive.google.com/uc?export=view&id=1m1_32ovMfjkRVb-B_3gPizDI1QUfZjQ4\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q5:</font>\n",
        "<br><font color='green'>\n",
        "Explain why it makes more sense to use the following update equation for $X[w[i], w[j]]$ when word $w[j]$ of index $j$ is in the context word $w[i]$ of index $i$. \n",
        " \n",
        "\\begin{equation*}\n",
        "X[w[i], w[j]] \\longleftarrow X[w[i], w[j]] + \\frac{1}{|i-j|}\n",
        "\\end{equation*}\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "e1M45t0GD0xC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVnkACgmLcub"
      },
      "source": [
        "---\n",
        "**Solution**\n",
        "\n",
        "* By using the update: $X[w[i], w[j]] \\longleftarrow X[w[i], w[j]] + 1$ we make the assumption that all the words $w[j]$ are equally important as context vectors of center word $w[i]$\n",
        "\n",
        "* By using the update: $X[w[i], w[j]] \\longleftarrow X[w[i], w[j]] + \\frac{1}{|i-j|}$, we would like to give more weight to closer context words because they are more related to the center word. \n",
        "\n",
        "* Another benefit from the update $X[w[i], w[j]] \\longleftarrow X[w[i], w[j]] + \\frac{1}{|i-j|}$ is to reduce the impact of the `context_size` hyperparameter. \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q6:</font>\n",
        "<br><font color='green'>\n",
        "Implement Algorithm 2 to get the co-occurence matrix $X$ using a function called `get_cooccurence_matrix()`.\n",
        "\n",
        "The function takes as arguments `sequences`, `context_size` and `vocabulary_size` and outputs the matrix `X`.\n",
        "</font>\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<center><img width=\"400\" src = \"https://drive.google.com/uc?export=view&id=1Xt6SXVNxlsPHqIWk1IIaLOZcpTrQWZSd\"></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "XI-bKFPhD2Bq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_occurence_matrix(sentences, context_size, vocabulary_size):\n",
        "  \"\"\"\n",
        "  This functions aims at creating the co-occurence matrix from the corpus\n",
        "  composed of sentences\n",
        "  \"\"\"\n",
        "  X = np.zeros((vocabulary_size, vocabulary_size))\n",
        "  N = len(sentences)\n",
        "  print(\"number of sentences to process:\", N)\n",
        "  it = 0\n",
        "  for sentence in sentences:\n",
        "      it += 1\n",
        "      if it % 10000 == 0:\n",
        "          print(\"processed\", it, \"/\", N)\n",
        "      n = len(sentence)\n",
        "      for i in range(n):\n",
        "          # center word\n",
        "          w_i = sentence[i]\n",
        "\n",
        "          start = max(0, i - context_size)\n",
        "          end = min(n-1, i + context_size)\n",
        "\n",
        "          # we can either choose only one side as context, or both\n",
        "          # here we are doing both\n",
        "          \n",
        "          # left context side\n",
        "          for j in range(start, i):\n",
        "              # context word\n",
        "              w_j = sentence[j]\n",
        "              # inverse of distance between w_i and w_j\n",
        "              inverse_distance = 1. / (i-j)\n",
        "              # Add the inverse of the distance to X[w_i, w_j]\n",
        "              X[w_i, w_j] += inverse_distance\n",
        "          \n",
        "          # right context side\n",
        "          for j in range(i+1, end+1):\n",
        "              # context word\n",
        "              w_j = sentence[j]\n",
        "              # inverse of distance between w_i and w_j\n",
        "              inverse_distance = 1. / (j-i)\n",
        "              # Add the inverse of the distance to X[w_i, w_j]\n",
        "              X[w_i, w_j] += inverse_distance    \n",
        "  return X"
      ],
      "metadata": {
        "id": "Hlpk4oBmHfNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcufa32cNtgM",
        "outputId": "05cf367f-ae58-4f82-f16a-9fada841a263"
      },
      "source": [
        "print(\"Get the co-occurence matrix X...\")\n",
        "X = get_occurence_matrix(sequences, C, V)\n",
        "print(\"The shape of X is\", X.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get the co-occurence matrix X...\n",
            "number of sentences to process: 73608\n",
            "processed 10000 / 73608\n",
            "processed 20000 / 73608\n",
            "processed 30000 / 73608\n",
            "processed 40000 / 73608\n",
            "processed 50000 / 73608\n",
            "processed 60000 / 73608\n",
            "processed 70000 / 73608\n",
            "The shape of X is (1000, 1000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zM372IuTZlr"
      },
      "source": [
        "\n",
        "\n",
        "Since non-zero values in the matrix $X$ are very large, we apply the logarithm function to all the elements of $X$ (after adding 1 to all the entries $X_{ij}$ to avoid applying the logarithm on zero values). The resulting matrix is still a sparse matrix. We will denote it $\\log X$.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q7:</font>\n",
        "<br><font color='green'>\n",
        "Create the matrix $\\log X$, call it `logX`.\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "U4c7UN5CD3ba"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8ANrmOAOEb8",
        "outputId": "f923aec0-0d45-4e6e-dfe5-8ad6b12bffc9"
      },
      "source": [
        "print(\"Get logX...\")\n",
        "logX = np.log(X+1)\n",
        "print(\"The shape of logX is\", logX.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get logX...\n",
            "The shape of logX is (1000, 1000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNTHnF-LkFFK"
      },
      "source": [
        "# 2. Training the weighted least squares regression model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHIrgh9bkFM-"
      },
      "source": [
        "## 2.1 Introducing the cost function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r59PLdpZkFXS"
      },
      "source": [
        "The logarithm of the co-occurence matrix $\\log X$ has been defined in the previous section. The objective of this section is to approximate $\\log X$ using a factorization method as follows:\n",
        "\n",
        "\\begin{equation*}\n",
        "\t\\forall (i,j) \\in \\{1, \\dots, V \\}^2 \\quad \\log X_{ij} \\approx W_i^T \\tilde{W}_j + b_i + \\tilde{b}_j\n",
        "\\end{equation*}\n",
        "\n",
        "\n",
        "The parameters of the regression model are:\n",
        "\n",
        "\n",
        "* A first **embedding matrix** and a bias term associated with it:\n",
        "\t\\begin{equation*}\n",
        "W = \\begin{pmatrix}\n",
        "- & W_1 & - \\\\\n",
        "\\vdots & \\vdots & \\vdots \\\\\n",
        "- & W_V & - \n",
        "\\end{pmatrix}\\in \\mathcal{M}_{V, D}(\\mathbb{R}), \t\\quad  \tb = \\begin{pmatrix}\n",
        " b_1  \\\\\n",
        " \\vdots  \\\\\n",
        " b_V  \n",
        "\\end{pmatrix}\\in \\mathbb{R}^{V}\n",
        "\t\\end{equation*}\t\t\n",
        "\t\n",
        "\n",
        "* A second **embedding matrix** and a bias term associated with it:\n",
        "\t\\begin{equation*}\n",
        "\\tilde{W} = \\begin{pmatrix}\n",
        "- & \\tilde{W}_1 & - \\\\\n",
        "\\vdots & \\vdots & \\vdots \\\\\n",
        "- & \\tilde{W}_V & - \n",
        "\\end{pmatrix}\\in \\mathcal{M}_{V, D}(\\mathbb{R}), \\quad \\tilde{b} = \\begin{pmatrix}\n",
        " \\tilde{b}_1  \\\\\n",
        " \\vdots  \\\\\n",
        " \\tilde{b}_V  \n",
        "\\end{pmatrix}\\in \\mathbb{R}^{V} \t\n",
        "\t\\end{equation*}\t\n",
        "\n",
        "\n",
        "Instead of equal-weighting all the co-occurences, we introduce a **weighting function** $f(X_{ij})$ defined as follows: \n",
        "\n",
        "\\begin{equation*}\n",
        "\\forall x \\in \\mathbb{R}_{+} \\quad f(x) =  \\begin{cases}\n",
        "      (x/x_{\\text{max}})^{\\alpha} & \\text{if   $x < x_{\\text{max}}$}\\\\\n",
        "      1 & \\text{otherwise}\n",
        "          \\end{cases}  \n",
        "\\end{equation*}\n",
        "\n",
        "\n",
        "The function $f$ is represented in the following figure with $x_{\\text{max}}=100$ and $\\alpha=0.75$\n",
        "\n",
        "<center><img width=\"400\" src = \"https://drive.google.com/uc?export=view&id=1D7muXkREj-5pPVUWyAfe8qrwYeIe0XzN\"></center>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q8:</font>\n",
        "<br><font color='green'>\n",
        "Create a matrix of shape $(V, V)$ whose entries are $f(X_{ij})$. \n",
        "Let's call it `fX`. Use the hyperparameters $x_{\\text{max}}=100$ and $\\alpha=0.75$\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "dBlWZ8ZsD4qx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEnJbfO6ObLg",
        "outputId": "abf37a23-3f82-4d32-8c5f-d1efc2eba325"
      },
      "source": [
        "print(\"Get f(X)...\")\n",
        "xmax = 100\n",
        "alpha = 0.75\n",
        "fX = np.zeros((V, V))\n",
        "fX[X < xmax] = (X[X < xmax] / float(xmax)) ** alpha\n",
        "fX[X >= xmax] = 1\n",
        "print(\"The shape of fX is\", fX.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get f(X)...\n",
            "The shape of fX is (1000, 1000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q9:</font>\n",
        "<br><font color='green'>\n",
        "What are the hyperparameters associated with the weighting function and what is the intuition behind introducing it?\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "1ofPy91eD5UN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWbvjvDsO8Cy"
      },
      "source": [
        "---\n",
        "**Solution:**\n",
        "* There are two hyperparameters associated with the weighting function: $x_{\\text{max}}$ and $\\alpha$\n",
        "\n",
        "* By introducing the weighting function, we make the assumption that rare occurences are noisy and carry less information than the more frequent ones. Therefore, when the entry $X_{ij}$ is small, we would like to reduce the contribution of the loss term associated with it (i.e, $(\\log X_{ij} - W_i^T \\tilde{W}_j - b_i - \\tilde{b}_j)^2$) to the global loss. \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjWn9PfcTzqW"
      },
      "source": [
        "The **cost function** can then be written as follows: \n",
        "\n",
        "\\begin{equation*}\n",
        "J = \\sum_{i=1}^V \\sum_{j=1}^V f(X_{ij}) (\\log X_{ij} - W_i^T \\tilde{W}_j - b_i - \\tilde{b}_j)^2\n",
        "\\end{equation*}\n",
        "\n",
        "\n",
        "The gradients of the cost function $J$ with respect to all the parameters are introduced in the following equations:\n",
        "\n",
        "\n",
        "For all $i \\in \\{1, \\dots, V \\}$ and all $j \\in \\{ 1, \\dots, V \\}$:\n",
        "\n",
        "\n",
        "\\begin{align} \n",
        "& \\nabla_{W_i} J(W_i) = -2 \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - b_i - \\tilde{b}_{j'} \\right) \\tilde{W}_{j'} \\quad \\text{(2.1)} \\\\\n",
        "& \\nabla_{\\tilde{W}_j} J(W_j) = -2 \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'} - \\tilde{b}_j \\right) W_{i'} \\quad \\text{(2.2)}  \\\\\n",
        "&\\nabla_{b_i} J(b_i) = -2 \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - b_i - \\tilde{b}_{j'} \\right) \\quad \\text{(2.3)}  \\\\\n",
        "& \\nabla_{\\tilde{b}_j} J(\\tilde{b}_j) = -2 \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'} - \\tilde{b}_j \\right) \\quad \\text{(2.4)} \n",
        "\\end{align}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q10:</font>\n",
        "<br><font color='green'>\n",
        "What is the total number of parameters in the model ? What are the shapes of all the gradients introduced in the equations (2.1), (2.2), (2.3) and (2.4) ?\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "XbzQWdyLD6Q2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDaCk9RMdYAr"
      },
      "source": [
        "---\n",
        "**Solution:**\n",
        "\n",
        "* Number of parameters:\n",
        "  * The matrices $W$ and $\\tilde{W}$ are both in $\\mathcal{M}_{V, D}(\\mathbb{R})$.\n",
        "  * The vectors $b$ and $\\tilde{b}$ are both $V$-dimensional vectors.\n",
        "  * So the total number of parameters $N_{\\text{parameters}}$ is \n",
        "\n",
        "  \\begin{equation}\n",
        "    N_{\\text{parameters}} = 2 (V D) + 2 V\n",
        "  \\end{equation}\n",
        "\n",
        "* Shapes of gradients:\n",
        "\n",
        "  * For all $i \\in \\{1, \\dots, V \\}$ and for all $j \\in \\{1, \\dots, V \\}$:\n",
        "\n",
        "\\begin{align} \n",
        "& \\nabla_{W_i} J(W_i) \\in \\mathcal{M}_{D, 1}(\\mathbb{R}) \\\\\n",
        "& \\nabla_{\\tilde{W}_j} J(W_j) \\in \\mathcal{M}_{D, 1}(\\mathbb{R})  \\\\\n",
        "&\\nabla_{b_i} J(b_i)  \\in \\mathbb{R}  \\\\\n",
        "& \\nabla_{\\tilde{b}_j} J(\\tilde{b}_j) \\in \\mathbb{R}\n",
        "\\end{align}\n",
        "    \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rZEuCrPT3cH"
      },
      "source": [
        "Let us introduce two training methods:\n",
        "\n",
        "* The first training method is called **alternating least squares**. It consists in finding the update equations by setting all the gradients to zero.\n",
        "* The second training method consists in applying the **gradient descent** algorithm.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbPZ_vBQkFdg"
      },
      "source": [
        "## 2.2 Alternating least squares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-i1DbdJkFib"
      },
      "source": [
        "We would like to estimate the parameters $W, \\tilde{W}, b, \\tilde{b}$ by setting the gradients to zero. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "By setting the gradients to zero, we get the following update equations: \n",
        "\n",
        "\\begin{align*}\n",
        "&\\nabla_{W_i} J(W_i) = 0 \\iff W_i = \\left( \\sum_{j'=1}^V f(X_{ij'}) \\tilde{W}_{j'} \\tilde{W}_{j'}^T \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - b_i - \\tilde{b}_{j'}) \\tilde{W}_{j'} \\right)  \\\\\n",
        "&\\nabla_{\\tilde{W}_j} J(\\tilde{W}_j) = 0 \\iff \\tilde{W}_j = \\left( \\sum_{i'=1}^V f(X_{i' j}) W_{i'} W_{i'}^T \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - b_{i'} - \\tilde{b}_{j}) W_{i'} \\right)  \\\\\n",
        "&\\nabla_{b_i} J(b_i) = 0 \\iff b_i = \\left( \\sum_{j'=1}^V f(X_{ij'})  \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - \\tilde{b}_{j'}) \\right)  \\\\\n",
        "&\\nabla_{\\tilde{b}_j} J(\\tilde{b}_j) = 0 \\iff \\tilde{b}_j = \\left( \\sum_{i'=1}^V f(X_{i' j})  \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - W_{i'}^T \\tilde{W}_{j} - b_{i'}) \\right) \n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "The proof can be found in **appendix A**"
      ],
      "metadata": {
        "id": "InI0dBGgD7tN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO0xwC4eXwAp"
      },
      "source": [
        "Each update equation for one parameter is a function of the other parameters. Therefore, in order to train our model, we can choose a number of iterations $N_{\\text{epochs}}$, and apply the update equations $N_{\\text{epochs}}$ times by keeping track of the loss to make sure it converges. \n",
        "\n",
        "For each iteration step $t \\in \\{0, \\dots, N_{\\text{epochs}}-1 \\}$, let $W^{(t)}, \\tilde{W}^{(t)}, b^{(t)}, \\tilde{b}^{(t)}$ represent the parameters of our model at the iteration $t$. \n",
        "\n",
        "\n",
        "The update equations from iteration $t$ to $t+1$ can then be written as follows:\n",
        "\n",
        "\\begin{align}\n",
        "&W_i^{(t+1)} \\longleftarrow \\left( \\sum_{j'=1}^V f(X_{ij'}) \\tilde{W}_{j'}^{(t)} \\tilde{W}_{j'}^{(t)^T} \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - b_i^{(t)} - \\tilde{b}_{j'}^{(t)}) \\tilde{W}_{j'}^{(t)} \\right) \\quad \\text{(2.5)} \\\\\n",
        "&\\tilde{W}_j^{(t+1)} \\longleftarrow \\left( \\sum_{i'=1}^V f(X_{i' j}) W_{i'}^{(t)} W_{i'}^{(t)^T} \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - b_{i'}^{(t)} - \\tilde{b}_{j}^{(t)}) W_{i'}^{(t)} \\right) \\quad \\text{(2.6)}  \\\\\n",
        "&b_i^{(t+1)} \\longleftarrow \\left( \\sum_{j'=1}^V f(X_{ij'})  \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - W_i^{(t)^T} \\tilde{W}_{j'}^{(t)} - \\tilde{b}_{j'}^{(t)}) \\right) \\quad \\text{(2.7)} \\\\\n",
        "&\\tilde{b}_j^{(t+1)} \\longleftarrow \\left( \\sum_{i'=1}^V f(X_{i' j})  \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - W_{i'}^{(t)^T} \\tilde{W}_{j}^{(t)} - b_{i'}^{(t)}) \\right) \\quad \\text{(2.8)}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\n",
        "The pseudo code for the training algorithm can be expressed as follows: \n",
        "\n",
        "\n",
        "<center><img width=\"500\" src = \"https://drive.google.com/uc?export=view&id=1DQmP3N13RH2hAP2-Szgk8TPhzcHICwGU\"></center>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q11:</font>\n",
        "<br><font color='green'>\n",
        "Implement the alternating least squares training algorithm\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "my9j51piD8-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimization Hyperparameters\n",
        "learning_rate=1e-4   \n",
        "epochs = EPOCHS\n",
        "\n",
        "# initialize weights\n",
        "W = np.random.randn(V, D) / np.sqrt(V + D)\n",
        "b = np.zeros(V)\n",
        "W_tilde = np.random.randn(V, D) / np.sqrt(V + D)\n",
        "b_tilde = np.zeros(V)\n",
        "\n",
        "\n",
        "\n",
        "costs = []\n",
        "for epoch in range(epochs):\n",
        "    # epsilon (V, V) matrix such that epsilon_{ij} = logX_{ij} - W_i^T W_tilde_j - b_i - b_tilde_j\n",
        "    epsilon = logX - W.dot(W_tilde.T) - b.reshape(V, 1) - b_tilde.reshape(1, V)\n",
        "    # cost function sum_{ij} fX_{ij} (logX_{ij} - W_i^T W_tilde_j - b_i - b_tilde_j)^2 = sum_{ij} fX_{ij} epsilon_{ij}^2\n",
        "    cost = (fX * epsilon * epsilon).sum()\n",
        "    costs.append(cost)\n",
        "    print(\"epoch: {}...cost: {}\".format(epoch, cost))\n",
        "    \n",
        "    # update W\n",
        "    print(\"Update W..\")\n",
        "    for i in range(V):\n",
        "        if i%1000==0:\n",
        "            print(\"Epoch {}... W is updated for {} words out of {}\".format(epoch, i, V))\n",
        "        # A = sum_{j'} fX_{ij'} W_tilde_j'  W_tilde_j'^T    \n",
        "        A = (fX[i,:][None, :]*W_tilde.T).dot(W_tilde) # or A = (W_tilde.T).dot(W_tilde*(fX[i, :][:, None]))\n",
        "        # B = sum_{j'} fX_{ij'} (logX_{ij'} - b_i - b_tilde_j') W_tilde_j'  \n",
        "        B = (fX[i, :]*(logX[i, :] - b[i] - b_tilde)).dot(W_tilde)\n",
        "        # W_i = A^{-1} B\n",
        "        W[i] = np.linalg.solve(A, B)\n",
        "\n",
        "\n",
        "    # update b\n",
        "    print(\"Update b..\")\n",
        "    for i in range(V):\n",
        "        if i%1000==0:\n",
        "            print(\"Epoch {}... b is updated for {} words out of {}\".format(epoch, i, V))\n",
        "        # A = sum_{j'} fX_{ij'}  \n",
        "        A = fX[i, :].sum()\n",
        "        # B = sum_{j'} fX_{ij'} (logX_{ij'} - W_i^T W_tilde_j' - b_tilde_j') \n",
        "        B = fX[i, :].dot(logX[i, :] - W[i].dot(W_tilde.T) - b_tilde)\n",
        "        # b_i = A^{-1} B\n",
        "        b[i] = B/A\n",
        "\n",
        "    # update W_tilde\n",
        "    print(\"Update W_tilde..\")\n",
        "    for j in range(V):\n",
        "        if j%1000==0:\n",
        "            print(\"Epoch {}... W_tilde is updated for {} words out of {}\".format(epoch, j, V))\n",
        "        # A = sum_{i'} fX_{i'j} W_i'  W_i'^T    \n",
        "        A = (fX[:, j][None, :]*(W.T)).dot(W)\n",
        "        # B = sum_{i'} fX_{i'j} (logX_{i'j} - b_i' - b_tilde_j) W_i' \n",
        "        B = (fX[:, j]*(logX[:, j] - b - b_tilde[j])).dot(W)\n",
        "        # W_tilde_j = A^{-1} B\n",
        "        W_tilde[j] = np.linalg.solve(A, B)                \n",
        "        \n",
        "        \n",
        "    # update b_tilde\n",
        "    print(\"Update b_tilde..\")\n",
        "    for j in range(V):\n",
        "        if j%1000==0:\n",
        "            print(\"Epoch {}... b_tilde is updated for {} words out of {}\".format(epoch, j, V))\n",
        "        # A = sum_{i'} fX_{i'j}  \n",
        "        A = fX[:, j].sum()\n",
        "        # B = sum_{i'} fX_{i'j} (logX_{i'j} - W_i'^T W_tilde_j - b_i') \n",
        "        B = fX[:, j].dot(logX[:, j] - W.dot(W_tilde[j]) - b)\n",
        "        # b_tilde_j = A^{-1} B\n",
        "        b_tilde[j] = B/A\n",
        "\n",
        "\n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62nQJ9gmaNk_",
        "outputId": "42527dda-c808-4cfb-cef2-8d0c75cdc637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0...cost: 436186.49119308114\n",
            "Update W..\n",
            "Epoch 0... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 0... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 0... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 0... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 1...cost: 5038.988345826602\n",
            "Update W..\n",
            "Epoch 1... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 1... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 1... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 1... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 2...cost: 2789.235340425804\n",
            "Update W..\n",
            "Epoch 2... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 2... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 2... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 2... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 3...cost: 2470.9653432880596\n",
            "Update W..\n",
            "Epoch 3... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 3... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 3... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 3... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 4...cost: 2356.923760074169\n",
            "Update W..\n",
            "Epoch 4... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 4... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 4... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 4... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 5...cost: 2301.7939336737613\n",
            "Update W..\n",
            "Epoch 5... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 5... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 5... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 5... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 6...cost: 2270.1916481534818\n",
            "Update W..\n",
            "Epoch 6... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 6... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 6... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 6... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 7...cost: 2249.7228241285893\n",
            "Update W..\n",
            "Epoch 7... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 7... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 7... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 7... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 8...cost: 2235.2531185105404\n",
            "Update W..\n",
            "Epoch 8... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 8... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 8... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 8... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 9...cost: 2224.3791276616785\n",
            "Update W..\n",
            "Epoch 9... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 9... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 9... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 9... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 10...cost: 2215.8331471468773\n",
            "Update W..\n",
            "Epoch 10... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 10... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 10... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 10... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 11...cost: 2208.8835694813115\n",
            "Update W..\n",
            "Epoch 11... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 11... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 11... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 11... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 12...cost: 2203.08583503121\n",
            "Update W..\n",
            "Epoch 12... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 12... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 12... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 12... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 13...cost: 2198.1600817697276\n",
            "Update W..\n",
            "Epoch 13... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 13... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 13... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 13... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 14...cost: 2193.9232876681162\n",
            "Update W..\n",
            "Epoch 14... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 14... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 14... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 14... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 15...cost: 2190.2497026685014\n",
            "Update W..\n",
            "Epoch 15... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 15... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 15... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 15... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 16...cost: 2187.0473593435513\n",
            "Update W..\n",
            "Epoch 16... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 16... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 16... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 16... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 17...cost: 2184.244305153161\n",
            "Update W..\n",
            "Epoch 17... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 17... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 17... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 17... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 18...cost: 2181.7810343146566\n",
            "Update W..\n",
            "Epoch 18... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 18... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 18... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 18... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 19...cost: 2179.6067776836285\n",
            "Update W..\n",
            "Epoch 19... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 19... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 19... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 19... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 20...cost: 2177.6778788278652\n",
            "Update W..\n",
            "Epoch 20... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 20... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 20... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 20... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 21...cost: 2175.95704330348\n",
            "Update W..\n",
            "Epoch 21... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 21... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 21... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 21... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 22...cost: 2174.412806541163\n",
            "Update W..\n",
            "Epoch 22... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 22... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 22... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 22... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 23...cost: 2173.0189626316806\n",
            "Update W..\n",
            "Epoch 23... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 23... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 23... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 23... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 24...cost: 2171.7539031231754\n",
            "Update W..\n",
            "Epoch 24... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 24... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 24... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 24... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 25...cost: 2170.599900178835\n",
            "Update W..\n",
            "Epoch 25... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 25... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 25... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 25... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 26...cost: 2169.5423939487655\n",
            "Update W..\n",
            "Epoch 26... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 26... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 26... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 26... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 27...cost: 2168.5693400514674\n",
            "Update W..\n",
            "Epoch 27... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 27... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 27... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 27... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 28...cost: 2167.670654482341\n",
            "Update W..\n",
            "Epoch 28... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 28... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 28... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 28... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 29...cost: 2166.8377708390867\n",
            "Update W..\n",
            "Epoch 29... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 29... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 29... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 29... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 30...cost: 2166.0633061146414\n",
            "Update W..\n",
            "Epoch 30... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 30... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 30... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 30... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 31...cost: 2165.340819726493\n",
            "Update W..\n",
            "Epoch 31... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 31... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 31... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 31... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 32...cost: 2164.6646455666078\n",
            "Update W..\n",
            "Epoch 32... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 32... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 32... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 32... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 33...cost: 2164.029776710697\n",
            "Update W..\n",
            "Epoch 33... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 33... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 33... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 33... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 34...cost: 2163.4317850441266\n",
            "Update W..\n",
            "Epoch 34... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 34... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 34... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 34... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 35...cost: 2162.8667619784374\n",
            "Update W..\n",
            "Epoch 35... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 35... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 35... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 35... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 36...cost: 2162.331270612377\n",
            "Update W..\n",
            "Epoch 36... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 36... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 36... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 36... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 37...cost: 2161.8223034128914\n",
            "Update W..\n",
            "Epoch 37... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 37... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 37... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 37... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 38...cost: 2161.3372423431147\n",
            "Update W..\n",
            "Epoch 38... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 38... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 38... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 38... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 39...cost: 2160.8738202447953\n",
            "Update W..\n",
            "Epoch 39... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 39... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 39... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 39... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 40...cost: 2160.4300833270377\n",
            "Update W..\n",
            "Epoch 40... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 40... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 40... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 40... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 41...cost: 2160.0043550657138\n",
            "Update W..\n",
            "Epoch 41... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 41... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 41... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 41... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 42...cost: 2159.5952019242736\n",
            "Update W..\n",
            "Epoch 42... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 42... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 42... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 42... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 43...cost: 2159.201401251844\n",
            "Update W..\n",
            "Epoch 43... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 43... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 43... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 43... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 44...cost: 2158.8219116116743\n",
            "Update W..\n",
            "Epoch 44... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 44... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 44... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 44... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 45...cost: 2158.4558456952864\n",
            "Update W..\n",
            "Epoch 45... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 45... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 45... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 45... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 46...cost: 2158.102445900459\n",
            "Update W..\n",
            "Epoch 46... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 46... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 46... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 46... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 47...cost: 2157.7610625912653\n",
            "Update W..\n",
            "Epoch 47... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 47... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 47... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 47... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 48...cost: 2157.431135006963\n",
            "Update W..\n",
            "Epoch 48... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 48... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 48... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 48... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 49...cost: 2157.1121747373536\n",
            "Update W..\n",
            "Epoch 49... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 49... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 49... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 49... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 50...cost: 2156.803751633137\n",
            "Update W..\n",
            "Epoch 50... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 50... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 50... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 50... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 51...cost: 2156.5054819735333\n",
            "Update W..\n",
            "Epoch 51... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 51... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 51... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 51... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 52...cost: 2156.2170186743565\n",
            "Update W..\n",
            "Epoch 52... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 52... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 52... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 52... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 53...cost: 2155.938043293195\n",
            "Update W..\n",
            "Epoch 53... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 53... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 53... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 53... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 54...cost: 2155.6682595770435\n",
            "Update W..\n",
            "Epoch 54... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 54... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 54... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 54... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 55...cost: 2155.407388302826\n",
            "Update W..\n",
            "Epoch 55... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 55... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 55... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 55... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 56...cost: 2155.1551631803695\n",
            "Update W..\n",
            "Epoch 56... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 56... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 56... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 56... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 57...cost: 2154.9113276169364\n",
            "Update W..\n",
            "Epoch 57... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 57... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 57... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 57... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 58...cost: 2154.6756321773787\n",
            "Update W..\n",
            "Epoch 58... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 58... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 58... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 58... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 59...cost: 2154.447832609496\n",
            "Update W..\n",
            "Epoch 59... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 59... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 59... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 59... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 60...cost: 2154.2276883363056\n",
            "Update W..\n",
            "Epoch 60... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 60... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 60... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 60... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 61...cost: 2154.014961342954\n",
            "Update W..\n",
            "Epoch 61... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 61... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 61... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 61... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 62...cost: 2153.8094154047008\n",
            "Update W..\n",
            "Epoch 62... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 62... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 62... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 62... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 63...cost: 2153.6108156141017\n",
            "Update W..\n",
            "Epoch 63... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 63... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 63... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 63... b_tilde is updated for 0 words out of 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q12:</font>\n",
        "<br><font color='green'>\n",
        " Plot the list of losses at the end of each iteration in Algorithm 3.\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0cmFRYw-D95S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(10, 7))\n",
        "plt.plot(costs)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"cost\")\n",
        "plt.title(\"Cost function using ALS\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "L7br03pNbwRG",
        "outputId": "490e7400-fdeb-482b-9508-4b5f14f6d42a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2wAAAJwCAYAAAD1D+IFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABNwUlEQVR4nO3deXSV1b3/8U+SkwSlSbACYVAZZFJBKBERUaMiiANaleFWewkdXCJeqbf9KdL2XootdSpDBSl6UdSq2FbQWlERFCcaVKBCEESGJJUAAQSSADlJzsn394ecJzlkIAzm2XDer7W+i+Q5OyebPLLMZ+3vs3ecJBMAAAAAwDnxfk8AAAAAAFA7AhsAAAAAOIrABgAAAACOIrABAAAAgKMIbAAAAADgKAIbAAAAADiKwAYAAAAAjiKwAQAAAICjCGwAAAAA4CgCGwDAWZ06ddLChQu1d+9emZluvPFGv6dUqyVLlmjJkiV+T+OYmZkmTJjg9zQAANUQ2AAgRnXs2FGzZs3Spk2bVFpaqqKiIn300UcaO3asmjRpcty/3ymnnKIJEyYoMzOzwV/z7LPPqkePHvrVr36lH/7wh1q+fPlxn1dDnXPOOZowYYLatWvn2xxiQbdu3WRmKi0tVVpaWq1jlixZopycnMO+V//+/fXGG29oy5YtKi0tVX5+vl577TX94Ac/ON7TBoBvlVEURVGxVddee63t37/fdu/ebdOmTbOf/vSnNmbMGHvxxRetrKzMnnjiieP+PU8//XQzM5swYUKDxjdp0sTMzH7729/6/vOSZLfccouZmWVmZtZ4LTEx0RITE32f47FWcnKyJSQk+DqH3/3ud7Z161YrLS21n/zkJ7WOWbJkieXk5NT7PkOHDrVwOGwrVqywe++9137605/apEmT7MMPP7R3333X9581RVFUQysgAEBMad++vV566SXl5+fryiuv1Pbt273XZs6cqbPPPlvXXXedjzP8RosWLSRJe/fu9XciDVBRUeH3FI6LsrIyv6egW2+9VS+++KI6dOig2267TU899dRRvc9vfvMbrV27VhdddFGN+xP5bwsAThS+p0aKoiiq8WrmzJlmZtavX78GjU9ISLBf//rXtnHjRgsGg5abm2uTJk2ypKSkqHEZGRn21ltv2c6dO+3AgQO2efNme+qpp0yStWvXzmpT12rbhAkTaozNzc01STZnzhzv49q+pvo1M7Pp06fbjTfeaDk5ORYMBm3NmjV29dVX1/j6Nm3a2OzZs62goMCCwaBt3rzZZs6caYmJiZaVlVXr/COrbUuWLLElS5ZEvV+LFi1s9uzZtn37distLbXPPvvMRo4cGTUm8nP5xS9+Ybfffrv3M/7kk0/sggsuOOy9qe3vLMmbb7t27Rp0f6r/vKrfk8j7n3322TZnzhzbs2eP7d27155++mk75ZRTor62SZMm9sc//tF27txpxcXF9ve//93atGlzRKuq/fv3NzOzCy64wIYNG2ahUMjatm1bY1xDVthKS0vt6aef9v3fG0VR1LEWK2wAEGOGDBmiTZs2KTs7u0HjZ8+erVGjRulvf/ubJk+erL59++qXv/ylzjnnHN18882SvlmxePvtt7Vz50499NBD2rt3r9q3b++9vnPnTo0ePVqzZs3S/PnzNX/+fEnS6tWra/2e8+fP1969ezVt2jS9+OKLeuONN7Rv376j+vtecskluvnmmzVz5kyVlJRo7Nixmjdvns466yzt3r1bktS6dWt98sknatasmZ588kl98cUXatu2rYYOHapTTz1VH3zwgf74xz/qZz/7mSZNmqR169ZJkvfnoZo0aaL33ntPnTp10owZM5Sbm6thw4bp2WefVbNmzfTYY49Fjb/11luVkpKiJ554Qmam++67T/Pnz1fHjh0VCoWO6u9d3eHuz+H89a9/VW5ursaPH6/evXvr9ttv144dO3T//fd7Y5555hmNGDFCzz33nJYtW6bMzEwtWLDgiOZ52223aePGjVq+fLnWrFmjAwcO6Ac/+IH+8Ic/HNH7SFJ+fr4GDBigtm3bqqCg4Ii/HgBc4ntqpCiKohqnUlJSzMzslVdeadD4888/38zMnnzyyajrjzzyiJmZXX755SbJbrzxRjMzy8jIqPO9jvQZtuqrT9WvH+kKWzAYtI4dO3rXevToYWZmd911l3ftmWeesVAoVO/863uG7dAVtrFjx5qZ2a233updCwQCtnTpUisuLrbvfOc7UX/HnTt3WrNmzbyxQ4YMMTOz6667rt6fUUNX2BpyfyI/r9pW2GbPnh01bt68ebZz507v8+9973tmZjZlypSocU8//XSD73kgELCdO3dGPbP4/PPP27/+9a9af96HW2H70Y9+5N3/d955xyZOnGj9+/e3uLi44/pviqIo6tsudokEgBiSmpoqSSopKWnQ+GuvvVaSNGXKlKjrkydPliTvWbfIc2bXX3+9AgG3mjcWL16szZs3e5/n5OSoqKhIHTt2lCTFxcXp+9//vv7xj39oxYoVx+V7Xnvttdq2bZvmzp3rXQuFQnrssceUkpJSY6fMv/zlL1HP6n344YeS5M3xWB3r/Zk1a1bU5x9++KGaN2+ulJQUSdLgwYMlffMMZHXTp09v8Pe45ppr1Lx586if2dy5c9WrVy+de+65RzznOXPm6Oqrr9Z7772nSy65RP/7v/+rjz76SBs2bFC/fv2O+P0AwC8ENgCIIcXFxZLk/aJ9OO3atVM4HNbGjRujrhcWFmrPnj3eFvfvv/++Xn75Zf3mN7/Rrl279Oqrr2rUqFFKSko6vn+Bo/Dvf/+7xrU9e/botNNOk/RNu2BaWprWrFlz3L5nu3bttGHDBn2z+FUl0kJ56NEAh84xErAiczxWx3p/Dp3fnj17ouYX+e8kNzc3atyh/93U54c//KE2b96ssrIynX322Tr77LO1adMm7d+/X7fddluD36e6t99+W4MHD1azZs106aWXasaMGWrXrp1ef/11Nh4BcMIgsAFADCkpKVFBQYG6d+9+RF93aPCozbBhw3TRRRdpxowZatu2rebMmaMVK1aoadOmRzvdI5pLQkJCrdfD4XCt1+Pi4o7bnI7V0c7xSH4Wx3J/vu2fYUpKioYMGaKOHTtq48aNXq1bt05NmzbVrbfeekzvX1paqo8++kh33323fve73+m73/2urrnmmuMydwD4thHYACDGvP766+rUqZMuuuiiw47Nz89XQkKCOnfuHHW9ZcuWOu2005Sfnx91/eOPP9avf/1r9enTR7feequ6d++u//iP/5DUsNDXEHv27FGzZs1qXD/aA6137typoqKiw4bYI5l/fn6+OnfuXCPQdOvWzXv9eIisdB16wHRdP4v67s+xiPx30qFDh6jrnTp1atDX33zzzTrllFM0evRoDR06NKp+9atfqX379urfv/8xz1OSd/h669atj8v7AcC3jcAGADHmkUce0b59+zR79my1bNmyxusdO3bU2LFjJUlvvPGGJOmee+6JGvPzn/9ckrxdAGsLUJ999pkkKTk5WZJ04MCBOsceiU2bNqlZs2bq0aOHd61Vq1a66aabjur9zEyvvvqqhgwZooyMjDrH7d+/X1LD5v/GG2+odevWGjFihHctISFBd999t0pKSvT+++8f1VwPtWnTJknSZZdd5l079dRTlZWVFTWuIffnWCxcuFCSNGbMmKjrd999d4O+/oc//KE2bdqkJ554QvPmzYuqP/zhDyopKTnitsgrr7yy1uuR5zLXr19/RO8HAH5x68lwAMC3bvPmzbr11lv1l7/8RevWrdNzzz2nNWvWKCkpSRdffLGGDRumZ555RtI32+4/88wzuuOOO9SsWTO9//77uvDCCzVq1Ci98soreu+99yRJWVlZGjNmjF555RVt2rRJKSkpuv3221VUVOSFvmAwqM8//1wjRozQl19+qd27d2vNmjX6/PPPj2j+L730kh5++GG98soreuyxx3Tqqafqzjvv1Jdffllv4KrPL3/5Sw0aNEjvv/++nnzySa1bt06tW7fWsGHDdMkll6ioqEifffaZQqGQxo0bp7S0NJWVlendd9/Vzp07a7zfk08+qTvuuEPPPPOMMjIylJeXp6FDh+qSSy7Rz372s6M+ouBQb7/9tvLz8/XUU0/p0UcfVTgc1o9//GPt3LkzapWtIffnWKxcuVIvv/yy/vu//1unn366t61/ly5dJNW/Otm6dWtdccUVNY46iCgvL9fChQs1bNgwjR071jvmoEWLFvrVr35VY3xubq5efPFF/f3vf1dubq7+8Y9/aNOmTWratKmuuuoq3XDDDfrkk0/0j3/845j/3gDQWHzfqpKiKIpq/OrUqZM98cQTtnnzZgsGg1ZUVGQffvih3XXXXVGHYickJNj//M//2KZNm6ysrMzy8/NrHJzdq1cve+GFFywvL89KS0tt+/bt9tprr1nv3r2jvudFF11kn376qQWDwcNu917Xtv6S7KqrrrLVq1dbMBi0devW2a233lrvwdmHfn1ubq7NmTMn6tqZZ55pzzzzjBUWFlppaalt3LjRpk+fbomJid6Yn/zkJ7Zx40arqKho0MHZTz31lO3YscOCwaCtWrXKsrKyGvx3bOh2+N/73vcsOzvbgsGg5eXl2T333FNjW/+G3p+6tvU//fTTo8bVdjD3KaecYtOnT7ddu3ZZcXGxzZ8/3zp37mxmZvfdd1+d8//v//5vMzO74oor6hwzcuRIMzMbMmSI9/Ouy6JFi0ySjRgxwl588UXbsGGD7d+/3w4cOGBr1qyx3/72t96xChRFUSdCxR38AAAA4Ljq2bOnPvvsM91222168cUX/Z4OAJyQeIYNAAAcsyZNmtS4ds899ygcDuuDDz7wYUYAcHLgGTYAAHDM7rvvPmVkZGjJkiUKhUK65pprdO211+qJJ57Qli1b/J4eAJywaIkEAADH7KqrrtKECRN07rnn6jvf+Y7+/e9/689//rMmTZpU5zluAIDDI7ABAAAAgKN4hg0AAAAAHEVgAwAAAABHselII2vTpo1KSkr8ngYAAAAAn6WkpGjr1q31jiGwNaI2bdqooKDA72kAAAAAcETbtm3rDW0EtkYUWVlr27Ytq2wAAABADEtJSVFBQcFhcwGBzQclJSUENgAAAACHxaYjAAAAAOAoAhsAAAAAOIrABgAAAACOIrABAAAAgKMIbAAAAADgKAIbAAAAADiKwAYAAAAAjiKwAQAAAICjCGwAAAAA4CgCGwAAAAA4isAGAAAAAI4isAEAAACAowhsAAAAAOAoAhsAAAAAOIrABgAAAACOIrABAAAAgKMIbAAAAADgKAIbAAAAADiKwAYAAAAAjiKwAQAAAICjCGwx6s6nH9e9r7yg757Rxu+pAAAAAKhDwO8JwB8tO7RTavPTlXTKKX5PBQAAAEAdWGGLUeGKCklSIJHMDgAAALiKwBajwhUhSVJCYqLPMwEAAABQFwJbjAodXGEjsAEAAADuIrDFKFoiAQAAAPcR2GJUOHSwJTLAChsAAADgKgJbjOIZNgAAAMB9BLYYRUskAAAA4D4CW4wKs+kIAAAA4DwCW4wKRVoiA6ywAQAAAK4isMUoVtgAAAAA9xHYYlRVYGOFDQAAAHAVgS1GRVoiA6ywAQAAAM4isMUoWiIBAAAA9xHYYpR3cDYtkQAAAICzCGwxioOzAQAAAPcR2GJU1cHZBDYAAADAVQS2GBUKfRPY4gMJPs8EAAAAQF0IbDEqzC6RAAAAgPMIbDGKXSIBAAAA9xHYYlSIg7MBAAAA5xHYYhQtkQAAAID7CGwxquocNgIbAAAA4CoCW4wK0xIJAAAAOI/AFqM4hw0AAABwH4EtRoUOPsMWH2CFDQAAAHAVgS1GscIGAAAAuI/AFqN4hg0AAABwH4EtRkW29WeXSAAAAMBdBLYYFaIlEgAAAHAegS1G0RIJAAAAuI/AFqM4OBsAAABwH4EtRnnPsLGtPwAAAOAsAluM4hk2AAAAwH0EthgVeYaNg7MBAAAAdxHYYlSkJZIVNgAAAMBdBLYYVX2XyLi4OJ9nAwAAAKA2zgS2cePGycw0depU71pycrJmzJihXbt2qaSkRC+//LJatmwZ9XVnnnmmXn/9de3fv1+FhYV65JFHlJCQEDUmMzNTK1asUDAY1IYNG5SVlVXj+48ZM0a5ubkqLS3VsmXL1KdPn6jXGzKXE0nkGTaJtkgAAADAVU4EtgsuuEB33HGHVq1aFXV96tSpGjJkiIYNG6bMzEy1adNG8+fP916Pj4/XggULlJSUpIsvvlhZWVkaNWqUHnjgAW9M+/bttWDBAi1ZskS9evXStGnTNHv2bA0aNMgbM3z4cE2ZMkUTJ05U7969tWrVKi1cuFAtWrRo8FxONJGWSIm2SAAAAMBl5mc1bdrU1q9fbwMGDLAlS5bY1KlTTZKlpqZaWVmZ3XLLLd7Yrl27mplZ3759TZINHjzYQqGQtWzZ0htzxx132N69ey0xMdEk2UMPPWQ5OTlR33Pu3Ln25ptvep8vW7bMpk+f7n0eFxdnW7ZssXHjxjV4Lg2plJQUMzNLSUnx9WcuyeLi421yTrZNzsm2U9NSfZ8PRVEURVEURcVSNTQb+L7C9vjjj2vBggV65513oq5nZGQoKSlJixcv9q6tX79e+fn56tevnySpX79+ysnJ0Y4dO7wxCxcuVFpams477zxvTPX3iIyJvEdiYqIyMjKixpiZFi9e7I1pyFxqk5SUpJSUlKhyhVVWqjIclsTh2QAAAICrfA1sI0aMUO/evTV+/Pgar7Vq1UplZWUqKiqKul5YWKhWrVp5YwoLC2u8HnmtvjFpaWlq0qSJmjdvrkAgUOuY6u9xuLnUZvz48SouLvaqoKCgzrF+YKdIAAAAwG2+BbYzzjhDf/zjH3XbbbeprKzMr2l8qx588EGlpqZ61bZtW7+nFCXEWWwAAACA03wLbBkZGUpPT9fKlStVUVGhiooKXX755Ro7dqwqKipUWFio5ORkpaWlRX1denq6tm/fLknavn270tPTa7weea2+MUVFRQoGg9q1a5dCoVCtY6q/x+HmUpvy8nKVlJRElUsiW/sHEglsAAAAgIt8C2zvvPOOunfvrl69enn16aef6oUXXlCvXr20fPlylZeXa8CAAd7XdOnSRe3atVN2drYkKTs7Wz169IjazXHgwIEqKirS2rVrvTHV3yMyJvIeFRUVWrFiRdSYuLg4DRgwwBuzYsWKw87lRBRpieQZNgAAAMBdvu+QEqnqu0RKspkzZ1peXp5dfvnl1rt3b1u6dKktXbq0aseU+HhbvXq1vfXWW3b++efboEGDrLCw0CZNmuSNad++ve3bt88efvhh69q1q915551WUVFhgwYN8sYMHz7cSktLbeTIkdatWzebNWuW7d69O2r3ycPNpSHl0i6RkuyXb86zyTnZdmb3c32fC0VRFEVRFEXFUh1BNvB/spE6NLAlJyfbjBkz7Ouvv7Z9+/bZvHnzLD09PeprzjrrLFuwYIHt37/fduzYYY8++qglJCREjcnMzLSVK1daMBi0jRs3WlZWVo3vfdddd1leXp4Fg0FbtmyZXXjhhVGvN2Qux/GmNEqNe+0lm5yTbR2+d77vc6EoiqIoiqKoWKqGZoO4gx+gEaSkpKi4uFipqalOPM/2/+Y/r9adz9affvJf2vjJCr+nAwAAAMSMhmYD389hg394hg0AAABwG4EthrFLJAAAAOA2AlsMC4W+CWyssAEAAABuIrDFMK8lkoOzAQAAACcR2GJYpCWSFTYAAADATQS2GFYV2FhhAwAAAFxEYIthoYMtkQFW2AAAAAAnEdhiGC2RAAAAgNsIbDEsHIqcw0ZLJAAAAOAiAlsM4+BsAAAAwG0EthhWdXA2gQ0AAABwEYEthoUiz7AFEnyeCQAAAIDaENhiGC2RAAAAgNsIbDGMXSIBAAAAtxHYYliIg7MBAAAApxHYYliYg7MBAAAApxHYYljVOWwENgAAAMBFBLYY5j3DFqAlEgAAAHARgS2GcQ4bAAAA4DYCWwwLsa0/AAAA4DQCWwyjJRIAAABwG4EthoXZ1h8AAABwGoEthtESCQAAALiNwBbD2HQEAAAAcBuBLYbREgkAAAC4jcAWw8KhsCRaIgEAAABXEdhiGCtsAAAAgNsIbDEsxDNsAAAAgNMIbDGMc9gAAAAAtxHYYliYbf0BAAAApxHYYhjPsAEAAABuI7DFsFAoEthYYQMAAABcRGCLYZGWyPj4eMUnJPg8GwAAAACHIrDFsEhgk2iLBAAAAFxEYIthkWfYJNoiAQAAABcR2GJYOFS1wsZZbAAAAIB7CGwxLnJ4djxnsQEAAADOIbDFuEhbZIBn2AAAAADnENhiHIdnAwAAAO4isMW4UAVnsQEAAACuIrDFOFoiAQAAAHcR2GIcLZEAAACAuwhsMS6ytT+BDQAAAHAPgS3G0RIJAAAAuIvAFuO8TUcCrLABAAAAriGwxbiqZ9hYYQMAAABcQ2CLcWG29QcAAACcRWCLcVWBjRU2AAAAwDUEthgXOtgSGeAZNgAAAMA5BLYYR0skAAAA4C4CW4yrOoeNlkgAAADANQS2GBfZJTLAChsAAADgHAJbjKMlEgAAAHAXgS3GVR2cneDzTAAAAAAcisAW46oOzmaFDQAAAHANgS3G0RIJAAAAuIvAFuNCIQ7OBgAAAFxFYItx7BIJAAAAuIvAFuNoiQQAAADcRWCLcVWbjtASCQAAALiGwBbjwgefYaMlEgAAAHAPgS3GhdjWHwAAAHAWgS3Gec+wBWiJBAAAAFxDYItxVZuOENgAAAAA1xDYYhwtkQAAAIC7CGwxLrLCxqYjAAAAgHsIbDGOlkgAAADAXQS2GBcOhSXREgkAAAC4iMAW42iJBAAAANxFYItxIVoiAQAAAGcR2GIc57ABAAAA7iKwxbgw2/oDAAAAziKwxThaIgEAAAB3EdhiXFVLJCtsAAAAgGsIbDEu0hLJLpEAAACAewhsMa76wdlxcXE+zwYAAABAdQS2GBcOhbyP49kpEgAAAHAKgS3GhSqqAhttkQAAAIBbCGwxLtISKbHCBgAAALiGwBbjrLJSleGwJCnA1v4AAACAUwhs4PBsAAAAwFEENlQ7PJvABgAAALiEwAbvOTZaIgEAAAC3ENhASyQAAADgKAIbvLPYCGwAAACAWwhsoCUSAAAAcBSBDWw6AgAAADiKwIaqZ9g4OBsAAABwCoENXkskK2wAAACAWwhsUCgUCWyssAEAAAAuIbDBa4kMsMIGAAAAOIXABloiAQAAAEcR2FAtsNESCQAAALiEwAaFQ2FJtEQCAAAAriGwgZZIAAAAwFEENlQdnB1I8HkmAAAAAKojsKHq4GxW2AAAAACnENhASyQAAADgKAIbvJbIALtEAgAAAE4hsIGWSAAAAMBRBDbQEgkAAAA4isAGhUORFTZaIgEAAACX+BrYRo8erVWrVqmoqEhFRUX65z//qcGDB3uvJycna8aMGdq1a5dKSkr08ssvq2XLllHvceaZZ+r111/X/v37VVhYqEceeUQJCdHb02dmZmrFihUKBoPasGGDsrKyasxlzJgxys3NVWlpqZYtW6Y+ffpEvd6QuZyowt4zbKywAQAAAC7xNbBt2bJF999/vzIyMnTBBRfo3Xff1d///nede+65kqSpU6dqyJAhGjZsmDIzM9WmTRvNnz/f+/r4+HgtWLBASUlJuvjii5WVlaVRo0bpgQce8Ma0b99eCxYs0JIlS9SrVy9NmzZNs2fP1qBBg7wxw4cP15QpUzRx4kT17t1bq1at0sKFC9WiRQtvzOHmciIL8QwbAAAA4Cxzqb7++mv78Y9/bKmpqVZWVma33HKL91rXrl3NzKxv374myQYPHmyhUMhatmzpjbnjjjts7969lpiYaJLsoYcespycnKjvMXfuXHvzzTe9z5ctW2bTp0/3Po+Li7MtW7bYuHHjTFKD5tKQSklJMTOzlJQU33/O1eviETfb5JxsGzl5ku9zoSiKoiiKoqhYqIZmA2eeYYuPj9eIESPUtGlTZWdnKyMjQ0lJSVq8eLE3Zv369crPz1e/fv0kSf369VNOTo527NjhjVm4cKHS0tJ03nnneWOqv0dkTOQ9EhMTlZGRETXGzLR48WJvTEPmUpukpCSlpKRElYuqNh3hGTYAAADAJb4Htu7du6ukpERlZWWaNWuWbrrpJq1bt06tWrVSWVmZioqKosYXFhaqVatWkqRWrVqpsLCwxuuR1+obk5aWpiZNmqh58+YKBAK1jqn+HoebS23Gjx+v4uJirwoKChr6Y2lUtEQCAAAAbvI9sK1fv169evVS37599ac//UnPPvuszjnnHL+ndVw8+OCDSk1N9apt27Z+T6lW3qYjAQIbAAAA4BLfe+AqKiq0adMmSdLKlSvVp08f/exnP9Nf/vIXJScnKy0tLWplKz09Xdu3b5ckbd++XRdeeGHU+6Wnp3uvRf6MXKs+pqioSMFgULt27VIoFKp1TPX3ONxcalNeXq7y8vIj+nn4gZZIAAAAwE2+r7AdKj4+XsnJyVqxYoXKy8s1YMAA77UuXbqoXbt2ys7OliRlZ2erR48eUbs5Dhw4UEVFRVq7dq03pvp7RMZE3qOiokIrVqyIGhMXF6cBAwZ4YxoylxNZOBSWREskAAAA4CLfdkb5/e9/b5deeqm1a9fOunfvbr///e8tHA7bVVddZZJs5syZlpeXZ5dffrn17t3bli5dakuXLq3aMSU+3lavXm1vvfWWnX/++TZo0CArLCy0SZOqdjts37697du3zx5++GHr2rWr3XnnnVZRUWGDBg3yxgwfPtxKS0tt5MiR1q1bN5s1a5bt3r07avfJw82lIeXqLpFdL+5rk3Oy7ed/fdb3uVAURVEURVFULNQRZAP/Jjl79mzLzc21YDBohYWFtmjRIi+sSbLk5GSbMWOGff3117Zv3z6bN2+epaenR73HWWedZQsWLLD9+/fbjh077NFHH7WEhISoMZmZmbZy5UoLBoO2ceNGy8rKqjGXu+66y/Ly8iwYDNqyZcvswgsvjHq9IXM5jjelUevsPr1tck623fvKC77PhaIoiqIoiqJioRqaDeIOfoBGkJKSouLiYqWmpqqkpMTv6Xja9zpfd//5Ce3M+7ceGjLC7+kAAAAAJ72GZgPnnmFD46vadIRn2AAAAACXENigELtEAgAAAE4isKHqHDZW2AAAAACnENigcEVIEi2RAAAAgGsIbODgbAAAAMBRBDYoHGKFDQAAAHARgQ0KHWyJjI+PV3xCgs+zAQAAABBBYIPXEinRFgkAAAC4hMCG6MAWILABAAAAriCwwXuGTeI5NgAAAMAlBDZIqn54NoENAAAAcAWBDZKqH55NSyQAAADgCgIbJHF4NgAAAOAiAhskcRYbAAAA4CICGyTREgkAAAC4iMAGSWw6AgAAALiIwAZJ1Z5h4xw2AAAAwBkENkiqaolkhQ0AAABwB4ENkqq3RLLCBgAAALiCwAZJVS2RAVbYAAAAAGcQ2CCJlkgAAADARQQ2SKoe2GiJBAAAAFxBYIMkKRwKS5ICAVbYAAAAAFcQ2CCJc9gAAAAAFxHYIKl6S2SCzzMBAAAAEEFgg6TqB2ezwgYAAAC4gsAGSewSCQAAALiIwAZJVc+wBdglEgAAAHAGgQ2SqrVEssIGAAAAOIPABkm0RAIAAAAuIrBBkhQORVbYaIkEAAAAXEFgg6SqFbYAK2wAAACAMwhskCSFeIYNAAAAcA6BDZKqPcMWoCUSAAAAcAWBDZKkcCiy6QiBDQAAAHAFgQ2SaIkEAAAAXERggyQ2HQEAAABcRGCDpOrnsNESCQAAALiCwAZJtEQCAAAALiKwQZJUefDgbFoiAQAAAHcQ2CBJCtESCQAAADiHwAZJUjjSEsk5bAAAAIAzCGyQVH3TEVoiAQAAAFcQ2CCJlkgAAADARQQ2SOIcNgAAAMBFBDZIqvYMG4ENAAAAcAaBDZJoiQQAAABcRGCDpKpz2BICrLABAAAAriCwQZIUquDgbAAAAMA1BDZIqr6tPy2RAAAAgCsIbJBUFdgkDs8GAAAAXEFgg6SqlkiJnSIBAAAAVxDYIOmQFTYCGwAAAOAEAhskSVZZqcpwWJIU4Dk2AAAAwAkENng4PBsAAABwC4ENnqrDswlsAAAAgAsIbPBEDs+mJRIAAABwA4ENHlbYAAAAALcQ2ODxnmHjHDYAAADACQQ2eMKssAEAAABOIbDBU9USyQobAAAA4AICGzyRlsgAK2wAAACAEwhs8NASCQAAALiFwAZPKERLJAAAAOASAhs8lRVhSbREAgAAAK4gsMFTtcJGYAMAAABcQGCDJ8wukQAAAIBTCGzwcHA2AAAA4BYCGzzsEgkAAAC4hcAGT+Tg7AAtkQAAAIATCGzweC2RrLABAAAATiCwwUNLJAAAAOCWowps//mf/6mkpKQa1xMTE/Wf//mfxzwp+IOWSAAAAMAtRxXY5syZo7S0tBrXU1JSNGfOnGOeFPxRGaIlEgAAAHDJUQW2uLg4mVmN62eccYaKioqOeVLwR4hn2AAAAACnHFHv28qVK2VmMjO98847Ch1ckZGkhIQEdejQQW+99dZxnyQah/cMG+ewAQAAAE44ot/MX331VUlSr169tHDhQu3bt897rby8XHl5eZo3b95xnSAaT9WmIwQ2AAAAwAVH9Jv5Aw88IEnKy8vTSy+9pPLy8m9lUvBHpCUyQEskAAAA4ISjeobt3XffVYsWLbzP+/Tpo6lTp+r2228/bhND42NbfwAAAMAtRxXYXnzxRV1xxRWSpPT0dC1evFgXXnihJk2apP/5n/85rhNE46ElEgAAAHDLUQW27t2765NPPpEkDR8+XDk5Oerfv79uu+02jRo16njOD42IXSIBAAAAtxxVYEtMTFRZWZkk6aqrrtJrr70mSfriiy/UunXr4zc7NKrIOWyBAIENAAAAcMFRBbbPP/9co0eP1iWXXKKBAwd6W/m3adNGX3/99XGdIBpPiJZIAAAAwClHFdjGjRunO+64Q++9957mzp2r1atXS5JuuOEGr1USJ55wpCWSc9gAAAAAJxzVb+bvv/++mjdvrtTUVO3du9e7/uSTT+rAgQPHa25oZOwSCQAAALjlqJdSKisrFQgE1L9/f0nS+vXrlZ+ff9wmhsZHSyQAAADglqNqiTz11FP11FNPadu2bfrggw/0wQcfaOvWrZo9e7ZOOeWU4z1HNJIwB2cDAAAATjmqwDZlyhRlZmZqyJAhatasmZo1a6Ybb7xRmZmZmjx58vGeIxoJLZEAAACAW44qsN1yyy36yU9+orfeekslJSUqKSnRm2++qdtvv11Dhw493nNEI6ElEgAAAHDLUbdEFhYW1ri+Y8cOnXrqqcc8KfjDO4eNFTYAAADACUcV2LKzszVx4kQlJyd715o0aaIJEyYoOzv7uE0OjSsU2dafwAYAAAA44ah63+655x699dZb2rJli1atWiVJ6tmzp8rKyjRo0KDjOkE0njAtkQAAAIBTjuo38zVr1qhz58667bbb1K1bN0nS3Llz9cILLygYDB7XCaLxeIGNg7MBAAAAJxzVb+b333+/CgsLNXv27KjrP/rRj9SiRQs98sgjx2VyaFyRlsj4hATFxcfLKit9nhEAAAAQ247qGbY77rhDX3zxRY3rn3/+uUaPHn3Mk4I/IitsEs+xAQAAAC44qsDWqlUrbdu2rcb1nTt3qnXr1sc8KfijemAL8BwbAAAA4LujCmxfffWV+vfvX+N6//79tXXr1mOeFPwRPritv8QKGwAAAOCCo1pG+b//+z9NmzZNiYmJevfddyVJAwYM0COPPKLJkycf1wmicYUqKhRITCSwAQAAAA44qsD26KOP6vTTT9fMmTOVlJQkSQoGg3r44Yf10EMPHdcJonGFK0IKJCbSEgkAAAA44Kh/K7///vv129/+Vuecc45KS0u1YcMGlZeXH8+5wQfhUIWkU1hhAwAAABxwTMso+/fv1/Lly4/XXOCA8MGt/TmLDQAAAPDfUW06gpOXd3g2LZEAAACA73wNbPfff78++eQTFRcXq7CwUK+88oq6dOkSNSY5OVkzZszQrl27VFJSopdfflktW7aMGnPmmWfq9ddf1/79+1VYWKhHHnlECQkJUWMyMzO1YsUKBYNBbdiwQVlZWTXmM2bMGOXm5qq0tFTLli1Tnz59jnguJ7qQF9hoiQQAAAD85mtgy8zM1OOPP66LLrpIAwcOVGJiot5++22deuqp3pipU6dqyJAhGjZsmDIzM9WmTRvNnz/fez0+Pl4LFixQUlKSLr74YmVlZWnUqFF64IEHvDHt27fXggULtGTJEvXq1UvTpk3T7NmzNWjQIG/M8OHDNWXKFE2cOFG9e/fWqlWrtHDhQrVo0aLBczkZRFoiAwQ2AAAAwAnmSjVv3tzMzC699FKTZKmpqVZWVma33HKLN6Zr165mZta3b1+TZIMHD7ZQKGQtW7b0xtxxxx22d+9eS0xMNEn20EMPWU5OTtT3mjt3rr355pve58uWLbPp06d7n8fFxdmWLVts3LhxDZ7L4SolJcXMzFJSUnz/WddVv3j5OZuck22dL+rj+1woiqIoiqIo6mSthmYDp55hS0tLkyTt3r1bkpSRkaGkpCQtXrzYG7N+/Xrl5+erX79+kqR+/fopJydHO3bs8MYsXLhQaWlpOu+887wx1d8jMibyHomJicrIyIgaY2ZavHixN6YhczlUUlKSUlJSosp1IZ5hAwAAAJzhTGCLi4vTtGnT9NFHH+nzzz+XJLVq1UplZWUqKiqKGltYWKhWrVp5YwoLC2u8HnmtvjFpaWlq0qSJmjdvrkAgUOuY6u9xuLkcavz48SouLvaqoKCgwT8Pv1SGwpJoiQQAAABc4Exge/zxx9W9e3f9x3/8h99TOW4efPBBpaametW2bVu/p3RYbDoCAAAAuMOJwDZ9+nRdf/31uuKKK6JWobZv367k5GSvVTIiPT1d27dv98akp6fXeD3yWn1jioqKFAwGtWvXLoVCoVrHVH+Pw83lUOXl5SopKYkq17GtPwAAAOAO3wPb9OnTddNNN+nKK69UXl5e1GsrVqxQeXm5BgwY4F3r0qWL2rVrp+zsbElSdna2evToEbWb48CBA1VUVKS1a9d6Y6q/R2RM5D0qKiq0YsWKqDFxcXEaMGCAN6YhczkZcHA2AAAA4BbfdkZ5/PHHbc+ePXbZZZdZenq6V02aNPHGzJw50/Ly8uzyyy+33r1729KlS23p0qVVu6bEx9vq1avtrbfesvPPP98GDRpkhYWFNmnSJG9M+/btbd++ffbwww9b165d7c4777SKigobNGiQN2b48OFWWlpqI0eOtG7dutmsWbNs9+7dUbtPHm4uh6sTYZfIrKkP2uScbLto2Pd9nwtFURRFURRFnax1BNnAv0nWJSsryxuTnJxsM2bMsK+//tr27dtn8+bNs/T09Kj3Oeuss2zBggW2f/9+27Fjhz366KOWkJAQNSYzM9NWrlxpwWDQNm7cGPU9InXXXXdZXl6eBYNBW7ZsmV144YVRrzdkLsfppvhWP3zkAZuck22X3DrU97lQFEVRFEVR1MlaDc0GcQc/QCNISUlRcXGxUlNTnX2e7QeT/lcX3HCNXnv0Mb3/3Fy/pwMAAACclBqaDXx/hg1uCbNLJAAAAOAMAhuiRLb1D7BLJAAAAOA7AhuihEMHd4lkhQ0AAADwHYENUbxt/QlsAAAAgO8IbIjiPcPGOWwAAACA7whsiFK16QiBDQAAAPAbgQ1RQgdbIgO0RAIAAAC+I7AhCtv6AwAAAO4gsCFKOERLJAAAAOAKAhuihNglEgAAAHAGgQ1RKnmGDQAAAHAGgQ1RQrREAgAAAM4gsCGKd3A257ABAAAAviOwIQq7RAIAAADuILAhSoiDswEAAABnENgQJcymIwAAAIAzCGyIQkskAAAA4A4CG6LQEgkAAAC4g8CGKJEVNloiAQAAAP8R2BAlHApLoiUSAAAAcAGBDVHCtEQCAAAAziCwIYoX2Dg4GwAAAPAdgQ1RQge39aclEgAAAPAfgQ1R2HQEAAAAcAeBDVEigU2iLRIAAADwG4ENUSItkRJtkQAAAIDfCGyIErXCRmADAAAAfEVgQxSrrFRlZaUkKcDW/gAAAICvCGyooeosNlbYAAAAAD8R2FBDOLK1P5uOAAAAAL4isKGGqhU2AhsAAADgJwIbagjREgkAAAA4gcCGGiItkRyeDQAAAPiLwIYa2HQEAAAAcAOBDTWEeIYNAAAAcAKBDTXQEgkAAAC4gcCGGsKhg9v6E9gAAAAAXxHYUAPb+gMAAABuILChBg7OBgAAANxAYEMNoRC7RAIAAAAuILChhkhLZICWSAAAAMBXBDbU4LVEssIGAAAA+IrAhho4OBsAAABwA4ENNYRoiQQAAACcQGBDDZzDBgAAALiBwIYaeIYNAAAAcAOBDTV4z7BxDhsAAADgKwIbaqjadITABgAAAPiJwIYaQgdbIgO0RAIAAAC+IrChBrb1BwAAANxAYEMNtEQCAAAAbiCwoQZaIgEAAAA3ENhQQyXnsAEAAABOILChhhAtkQAAAIATCGyogYOzAQAAADcQ2FADB2cDAAAAbiCwoQZaIgEAAAA3ENhQQ6QlMhCgJRIAAADwE4ENNXBwNgAAAOAGAhtqoCUSAAAAcAOBDTWEOTgbAAAAcAKBDTWEOTgbAAAAcAKBDTWEaYkEAAAAnEBgQw2cwwYAAAC4gcCGGkIVtEQCAAAALiCwoYbIChubjgAAAAD+IrChBp5hAwAAANxAYEMNkZbI+IQExcXznwgAAADgF34bRw2RFTaJ59gAAAAAPxHYUEPkHDZJCtAWCQAAAPiGwIYaKqsFNlbYAAAAAP8Q2FCDmSkc2dqfs9gAAAAA3xDYUKsQO0UCAAAAviOwoVbhUCSw0RIJAAAA+IXAhlpFWiI5PBsAAADwD4ENteLwbAAAAMB/BDbUquoZNlbYAAAAAL8Q2FArWiIBAAAA/xHYUKvI4dmssAEAAAD+IbChVjzDBgAAAPiPwIZacXA2AAAA4D8CG2rFpiMAAACA/whsqFWkJTJASyQAAADgGwIbauW1RLLCBgAAAPiGwIZa0RIJAAAA+I/AhlrREgkAAAD4j8CGWnnnsAVYYQMAAAD8QmBDrXiGDQAAAPAfgQ214uBsAAAAwH8ENtQqRGADAAAAfEdgQ60iLZEBnmEDAAAAfENgQ63CbOsPAAAA+I7AhlrxDBsAAADgPwIbahWKtESywgYAAAD4hsCGWtESCQAAAPiPwIZaeQdn0xIJAAAA+IbAhlpxcDYAAADgPwIbauW1RAZYYQMAAAD8QmBDrUKhbwJbgJZIAAAAwDcENtSKlkgAAADAfwQ21IpdIgEAAAD/EdhQqxAHZwMAAAC+8zWwXXrppXrttddUUFAgM9ONN95YY8zEiRO1detWHThwQIsWLVKnTp2iXj/ttNP0/PPPq6ioSHv27NHs2bPVtGnTqDE9evTQBx98oNLSUv373//WvffeW+P7DB06VOvWrVNpaalWr16ta6655ojncjIJc3A2AAAA4DtfA1vTpk21atUq3XXXXbW+ft9992ns2LEaPXq0+vbtq/3792vhwoVKTk72xrzwwgs677zzNHDgQF1//fW67LLL9OSTT3qvp6Sk6O2331Z+fr4yMjJ077336je/+Y1uv/12b0y/fv00d+5cPfXUU/re976nV199Va+++qrOO++8I5rLyaTqHDYCGwAAAOAnc6HMzG688caoa1u3brVf/OIX3uepqalWWlpqI0aMMEnWrVs3MzPLyMjwxlx99dUWDoetdevWJslGjx5tX3/9tSUmJnpjHnzwQVu3bp33+UsvvWT/+Mc/or53dna2/elPf2rwXBpSKSkpZmaWkpLi+8/7cNWuZ3ebnJNt49/4m+9zoSiKoiiKoqiTrRqaDZx9hq1Dhw5q3bq1Fi9e7F0rLi7Wxx9/rH79+kn6ZmVsz549WrFihTdm8eLFqqysVN++fb0xH3zwgSoOPpMlSQsXLlS3bt3UrFkzb0z17xMZE/k+DZlLbZKSkpSSkhJVJwrOYQMAAAD852xga9WqlSSpsLAw6nphYaH3WqtWrbRjx46o18PhsHbv3h01prb3qP496hpT/fXDzaU248ePV3FxsVcFBQWH+Vu7I8S2/gAAAIDvnA1sJ4MHH3xQqampXrVt29bvKTVYZIWNTUcAAAAA/zgb2LZv3y5JSk9Pj7qenp7uvbZ9+3a1bNky6vWEhAR997vfjRpT23tU/x51jan++uHmUpvy8nKVlJRE1YkizLb+AAAAgO+cDWy5ubnatm2bBgwY4F1LSUlR3759lZ2dLUnKzs7Waaedpt69e3tjrrzySsXHx+vjjz/2xlx22WUKVHsWa+DAgfriiy+0d+9eb0z17xMZE/k+DZnLyYaWSAAAAMANvu2M0rRpU+vZs6f17NnTzMzuuece69mzp5155pkmye677z7bvXu3DRkyxLp3726vvPKKbdq0yZKTk733eOONN2zFihXWp08fu/jii239+vX2wgsveK+npqbatm3b7Nlnn7Vzzz3Xhg8fbvv27bPbb7/dG9OvXz8rLy+3n//859a1a1ebMGGClZWV2XnnneeNachcDlcn0i6RTU9rZpNzsm1yTrbvc6EoiqIoiqKok62OIBv4N8nMzEyrzZw5c7wxEydOtG3btllpaaktWrTIOnfuHPUep512mr3wwgtWXFxse/futaeeesqaNm0aNaZHjx72wQcfWGlpqX311Vd233331ZjL0KFD7YsvvrBgMGg5OTl2zTXX1BhzuLkcx5viezX5TlMvsCUEAr7Ph6IoiqIoiqJOpmpoNog7+AEaQUpKioqLi5Wamur882yJTZL10KfvSZLGX3ilyktL/Z0QAAAAcBJpaDZw9hk2+Ct88Bk2iY1HAAAAAL8Q2FCrynBYlZWVkghsAAAAgF8IbKiTdxZbgJ0iAQAAAD8Q2FCnMFv7AwAAAL4isKFOHJ4NAAAA+IvAhjqFvMDGChsAAADgBwIb6hRpiQwQ2AAAAABfENhQp3CIZ9gAAAAAPxHYUCeeYQMAAAD8RWBDnbxdIgMENgAAAMAPBDbUiU1HAAAAAH8R2FAn7+BsWiIBAAAAXxDYUCcOzgYAAAD8RWBDnUIhWiIBAAAAPxHYUCdaIgEAAAB/EdhQJ1oiAQAAAH8R2FAnDs4GAAAA/EVgQ528g7M5hw0AAADwBYENdao6h43ABgAAAPiBwIY6RZ5hC9ASCQAAAPiCwIY6eS2RBDYAAADAFwQ21ClMSyQAAADgKwIb6hSiJRIAAADwFYENdaIlEgAAAPAXgQ11qjqHjZZIAAAAwA8ENtSJXSIBAAAAfxHYUKdIS2Q8B2cDAAAAviCwoU6Rg7MDtEQCAAAAviCwoU6Rlkg2HQEAAAD8QWBDndglEgAAAPAXgQ11CnFwNgAAAOArAhvq5O0SGWCFDQAAAPADgQ11qjqHjcAGAAAA+IHAhjqFaYkEAAAAfEVgQ528wMY5bAAAAIAvCGyoU4ht/QEAAABfEdhQp7B3cDaBDQAAAPADgQ114hk2AAAAwF8ENtSJlkgAAADAXwQ21ImWSAAAAMBfBDbUiZZIAAAAwF8ENtQpcnB2fEKC4uL5TwUAAABobPwWjjqFDz7DJvEcGwAAAOAHAhvqFDrYEilJCYEEH2cCAAAAxCYCG+pUGapaYWPjEQAAAKDxEdhQJzPz2iJpiQQAAAAaH4EN9QqxUyQAAADgGwIb6hUORQIbK2wAAABAYyOwoV6RlkieYQMAAAAaH4EN9YqcxUZLJAAAAND4CGyoV7iClkgAAADALwQ21MvbJTLAChsAAADQ2AhsqFdkl0ieYQMAAAAaH4EN9QqzrT8AAADgGwIb6sXB2QAAAIB/CGyoV4hNRwAAAADfENhQr7D3DBstkQAAAEBjI7ChXrREAgAAAP4hsKFeVQdnE9gAAACAxkZgQ728XSI5hw0AAABodAQ21CvEtv4AAACAbwhsqFfkGTYOzgYAAAAaH4EN9QqzrT8AAADgGwIb6kVLJAAAAOAfAhvq5bVEBlhhAwAAABobgQ31oiUSAAAA8A+BDfWqOoeNlkgAAACgsRHYUC92iQQAAAD8Q2BDvSItkfEcnA0AAAA0OgIb6hXZJTJASyQAAADQ6AhsqFekJZJNRwAAAIDGR2BDvdglEgAAAPAPgQ31CoVoiQQAAAD8QmBDvWiJBAAAAPxDYEO9aIkEAAAA/ENgQ72qVthoiQQAAAAaG4EN9QoffIYtgXPYAAAAgEZHYEO9QjzDBgAAAPiGwIZ6hb2DswlsAAAAQGMjsKFeVZuO0BIJAAAANDYCG+pFSyQAAADgHwIb6kVLJAAAAOAfAhvqRUskAAAA4B8CG+oVDtESCQAAAPiFwIZ6RQ7OpiUSAAAAaHwENtQrdLAlUpLiAwk+zgQAAACIPQQ21CtcLbCxygYAAAA0LgIb6hVpiZR4jg0AAABobAQ21KsyHFZlZaUkdooEAAAAGhuBDYflncUWYIUNAAAAaEwENhxWpC2SlkgAAACgcRHYcFgcng0AAAD4g8CGw+LwbAAAAMAfBDYcVuQstoQAK2wAAABAYyKw4bAiz7BxDhsAAADQuAhsOCyeYQMAAAD8QWDDYbFLJAAAAOAPAhsOy3uGjcAGAAAANCoCGw7LOziblkgAAACgUfEbOA4r0hI55P/drXMuvVh5n+Uob1WOduTm+zwzAAAA4ORGYMNhfbF0mTpfdIFOP6OtTj+jrfrecoMkaf/eom/C28EA99WataoIlvk8WwAAAODkESfJ/J5ErEhJSVFxcbFSU1NVUlLi93SOSNNmaWrXs4fa9/qmzup+rhKbJEeNCVeEVLD+S321Zp327y1SqKxcFeVlqgiWKVRe/s3nZeWqKCtTqKxMFQevVYbD31RlpSxcqcrK8ME/K2WV3/xZGa6UVYZllSazSpmZzCSrrJTMDn5u33wOAAAAOK6h2YDAdoTGjBmje++9V61atdKqVat0991369NPP23Q157Ige1QCYGA2nTr4gW4Dr3OV1p6C7+nFaWyWpiTSSaL/jzysSSp2seRP7zPq/6JmBow5uDHZrX/06pxvZavrflFtV2qa2wDv+8Rfr+61PketbxJ3WOPbQ5H/N5H9sb+z+Fb5MKcXZjDETsR53wETsh7AjQi/o2c2PL+tVrzfveo39NocDagJfIIDB8+XFOmTNHo0aP18ccf65577tHChQvVtWtX7dy50+/pNapwKKSv1qzVV2vW6sPn/yJJatYqXe179VDbc7oo6ZRTlJiUpMQmyQokJSmQnKTEpORv/kyO/PnNx3Hx8YpPSFB8fLziEuIVH5+g+IR4xcXHKyFw9P+Jxsezpw4AAACiFe/Y5fcUjggrbEdg2bJl+vTTT3X33XdLkuLi4vTVV19p+vTpevjhhw/79SfTCltjiouL84JcXHzcN5/HxUtxB1+Lj5cUp/j4OCku7psxOvhxXOTPg+8Vdf2brz/4TaJer3ap2ufRf9Y2z+gvrGXsIZ9Hf3roa/V8XvsU6pxbnV/Q0Peo8+/c4Lc9osF1/z3qGH8Ef78jm/S39Lbf0hy+eetv772PYBJ+z+CInXgz1gn5c3aBE/9GEIV7gsZ2oKhYBV986fc0WGE73hITE5WRkaEHH3zQu2ZmWrx4sfr161fr1yQlJSk5ueo5r5SUlG99nicjM5OFwqpU2O+pAAAAAI2KnrEGat68uQKBgAoLC6OuFxYWqlWrVrV+zfjx41VcXOxVQUFBY0wVAAAAwEmCwPYtevDBB5WamupV27Zt/Z4SAAAAgBMILZENtGvXLoVCIaWnp0ddT09P1/bt22v9mvLycpWXlzfG9AAAAACchFhha6CKigqtWLFCAwYM8K7FxcVpwIABys7O9nFmAAAAAE5WrLAdgSlTpujZZ5/V8uXL9cknn+iee+5R06ZNNWfOHL+nBgAAAOAkRGA7An/961/VokULPfDAA2rVqpU+++wzDR48WDt27PB7agAAAABOQpzD1og4hw0AAACA1PBswDNsAAAAAOAoAhsAAAAAOIrABgAAAACOIrABAAAAgKMIbAAAAADgKAIbAAAAADiKwAYAAAAAjiKwAQAAAICjCGwAAAAA4CgCGwAAAAA4isAGAAAAAI4isAEAAACAowhsAAAAAOAoAhsAAAAAOCrg9wRiUUpKit9TAAAAAOCjhmYCAlsjityUgoICn2cCAAAAwAUpKSkqKSmp8/U4SdZ400GbNm3qvSGNJSUlRQUFBWrbtq0T88GR4f6d2Lh/Jzbu34mLe3di4/6d2Lh/tUtJSdHWrVvrHcMKWyM73A1pbCUlJfyjOYFx/05s3L8TG/fvxMW9O7Fx/05s3L9oDflZsOkIAAAAADiKwAYAAAAAjiKwxaiysjL95je/UVlZmd9TwVHg/p3YuH8nNu7fiYt7d2Lj/p3YuH9Hj01HAAAAAMBRrLABAAAAgKMIbAAAAADgKAIbAAAAADiKwAYAAAAAjiKwxagxY8YoNzdXpaWlWrZsmfr06eP3lFCLSy+9VK+99poKCgpkZrrxxhtrjJk4caK2bt2qAwcOaNGiRerUqZMPM8Wh7r//fn3yyScqLi5WYWGhXnnlFXXp0iVqTHJysmbMmKFdu3appKREL7/8slq2bOnTjFHd6NGjtWrVKhUVFamoqEj//Oc/NXjwYO917t2JY9y4cTIzTZ061bvG/XPbhAkTZGZRtW7dOu917p/b2rRpoz//+c/atWuXDhw4oNWrVysjIyNqDL+7HDmjYquGDx9uwWDQRo0aZeecc4498cQTtnv3bmvRooXvc6Oia/Dgwfbb3/7Wvv/975uZ2Y033hj1+n333Wd79uyxG264wXr06GGvvvqqbdq0yZKTk32fe6zXm2++aVlZWXbuuefa+eefb6+//rrl5eXZqaee6o2ZOXOm5efn2xVXXGG9e/e2f/7zn/bRRx/5PndKdv3119s111xjnTp1ss6dO9vvfvc7Kysrs3PPPZd7dwLVBRdcYJs3b7bPPvvMpk6d6l3n/rldEyZMsJycHEtPT/fq9NNP5/6dANWsWTPLzc21p59+2vr06WPt27e3gQMHWseOHb0x/O5yVOX7BKhGrmXLltn06dO9z+Pi4mzLli02btw43+dG1V21BbatW7faL37xC+/z1NRUKy0ttREjRvg+Xyq6mjdvbmZml156qXevysrK7JZbbvHGdO3a1czM+vbt6/t8qZr19ddf249//GPu3QlSTZs2tfXr19uAAQNsyZIlXmDj/rlfEyZMsH/961+1vsb9c7sefPBB++CDD+odw+8uR160RMaYxMREZWRkaPHixd41M9PixYvVr18/H2eGI9WhQwe1bt066l4WFxfr448/5l46KC0tTZK0e/duSVJGRoaSkpKi7t/69euVn5/P/XNMfHy8RowYoaZNmyo7O5t7d4J4/PHHtWDBAr3zzjtR17l/J4bOnTuroKBAmzZt0vPPP68zzzxTEvfPdTfccIOWL1+uv/71ryosLNTKlSv105/+1Hud312ODoEtxjRv3lyBQECFhYVR1wsLC9WqVSufZoWjEblf3Ev3xcXFadq0afroo4/0+eefS/rm/pWVlamoqChqLPfPHd27d1dJSYnKyso0a9Ys3XTTTVq3bh337gQwYsQI9e7dW+PHj6/xGvfPfR9//LFGjRqlwYMH684771SHDh304Ycf6jvf+Q73z3EdO3bUnXfeqQ0bNujqq6/Wn/70Jz322GMaOXKkJH53OVoBvycAACe7xx9/XN27d9cll1zi91RwBNavX69evXopLS1NQ4cO1bPPPqvMzEy/p4XDOOOMM/THP/5RAwcOVFlZmd/TwVF46623vI9zcnL08ccfKz8/X8OHD1dpaamPM8PhxMfHa/ny5frVr34lSfrss8/UvXt3jR49Ws8995zPsztxscIWY3bt2qVQKKT09PSo6+np6dq+fbtPs8LRiNwv7qXbpk+fruuvv15XXHGFCgoKvOvbt29XcnKy1yoZwf1zR0VFhTZt2qSVK1fql7/8pVatWqWf/exn3DvHZWRkKD09XStXrlRFRYUqKip0+eWXa+zYsaqoqFBhYSH37wRTVFSkL7/8Up06deLfn+O2bdumtWvXRl1bt26dzjrrLEn87nK0CGwxpqKiQitWrNCAAQO8a3FxcRowYICys7N9nBmOVG5urrZt2xZ1L1NSUtS3b1/upSOmT5+um266SVdeeaXy8vKiXluxYoXKy8uj7l+XLl3Url077p+j4uPjlZyczL1z3DvvvKPu3burV69eXn366ad64YUX1KtXLy1fvpz7d4Jp2rSpzj77bG3bto1/f45bunSpunbtGnWtS5cuys/Pl8TvLsfC951PqMat4cOHW2lpqY0cOdK6detms2bNst27d1vLli19nxsVXU2bNrWePXtaz549zczsnnvusZ49e9qZZ55p0jdb4+7evduGDBli3bt3t1deeYWtcR2pxx9/3Pbs2WOXXXZZ1NbUTZo08cbMnDnT8vLy7PLLL7fevXvb0qVLbenSpb7PnZL9/ve/t0svvdTatWtn3bt3t9///vcWDoftqquu4t6dgFV9l0jun/v16KOP2mWXXWbt2rWzfv362dtvv207duyw5s2bc/8crwsuuMDKy8tt/PjxdvbZZ9sPfvAD27dvn916663eGH53OaryfQKUD3XXXXdZXl6eBYNBW7ZsmV144YW+z4mqWZmZmVabOXPmeGMmTpxo27Zts9LSUlu0aJF17tzZ93lTqvW+mZllZWV5Y5KTk23GjBn29ddf2759+2zevHmWnp7u+9wp2ezZsy03N9eCwaAVFhbaokWLvLDGvTvx6tDAxv1zu+bOnWsFBQUWDAbtq6++srlz50ad48X9c7uuu+46W716tZWWltratWvtpz/9aY0x/O5yZBV38AMAAAAAgGN4hg0AAAAAHEVgAwAAAABHEdgAAAAAwFEENgAAAABwFIENAAAAABxFYAMAAAAARxHYAAAAAMBRBDYAAAAAcBSBDQAAB2VmZsrMlJaW5vdUAAA+IrABAAAAgKMIbAAAAADgKAIbAAC1iIuL0/3336/NmzfrwIED+uyzz3TLLbdIqmpXvPbaa7Vq1SqVlpYqOztb5513XtR73HzzzVqzZo2CwaByc3P185//POr1pKQkPfTQQ/r3v/+tYDCoDRs26Mc//nHUmIyMDH366afav3+/li5dqi5dunivnX/++Xr33XdVXFysoqIiLV++XBkZGd/STwQA4BejKIqiKCq6fvnLX9ratWtt0KBB1qFDB8vKyrLS0lK77LLLLDMz08zMPv/8c7vqqquse/fu9tprr9nmzZstEAiYJOvdu7eFQiH79a9/bZ07d7asrCzbv3+/ZWVled/jpZdesvz8fPv+979vHTp0sCuvvNKGDx9ukrzvkZ2dbZdddpmdc8459v7779tHH33kfX1OTo4999xz1rVrV+vUqZMNHTrUzj//fN9/dhRFUdRxLd8nQFEURVFOVVJSku3bt88uuuiiqOv/93//Zy+88IIXpiLhSpKddtpptn//fhs2bJhJsueff94WLlwY9fUPP/ywrVmzxiRZ586dzcxswIABtc4h8j2uvPJK79o111xjZmbJyckmyYqKimzkyJG+/7woiqKob69oiQQA4BCdOnVS06ZNtWjRIpWUlHg1cuRInX322d647Oxs7+M9e/Zo/fr1OueccyRJ55xzjpYuXRr1vkuXLlXnzp0VHx+vXr16KRQK6f333693LqtXr/Y+3rZtmySpZcuWkqQpU6Zo9uzZWrRokcaNG6eOHTse218cAOAcAhsAAIf4zne+I0m67rrr1KtXL6/OPfdcDR069Lh8j9LS0gaNq6io8D42M0lSfPw3//ueOHGizjvvPC1YsEBXXnml1q5dq+9///vHZX4AADcQ2AAAOMTatWsVDAZ11llnadOmTVG1ZcsWb9xFF13kfdysWTN16dJF69atkyStW7dO/fv3j3rf/v3768svv1RlZaVycnIUHx+vzMzMY5rrhg0bNG3aNF199dWaP3++fvSjHx3T+wEA3BLwewIAALhm3759+sMf/qCpU6cqPj5eH330kdLS0tS/f38VFxcrPz9fkvS///u/+vrrr1VYWKhJkyZp165devXVVyVJkydP1qeffqpf//rX+stf/qJ+/frpv/7rvzRmzBhJUn5+vp599lk9/fTTGjt2rFatWqV27dqpZcuW+tvf/nbYOTZp0kSPPvqoXn75ZeXm5uqMM85Qnz59NG/evG/t5wIA8IfvD9JRFEVRlIs1duxYW7dunZWVlVlhYaG9+eabdumll3obglx33XWWk5NjwWDQli1bZj169Ij6+ptvvtnWrFljZWVllpeXZ7/4xS+iXk9OTrbJkydbQUGBBYNB+/LLL23UqFEmVW06kpaW5o3v2bOnmZm1a9fOEhMT7cUXX7T8/HwLBoO2ZcsWe+yxx7wNSSiKoqiTo+IOfgAAABooMzNT7733npo1a6aioiK/pwMAOInxDBsAAAAAOIrABgAAAACOoiUSAAAAABzFChsAAAAAOIrABgAAAACOIrABAAAAgKMIbAAAAADgKAIbAAAAADiKwAYAAAAAjiKwAQAAAICjCGwAAAAA4Kj/D2ECS9DKypQpAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX56bVMHkFnd"
      },
      "source": [
        "## 2.3 Learning the weights using gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Luje3fIdkFsY"
      },
      "source": [
        "In this section, we would like to estimate the parameters of the model using gradient descent.\n",
        "\n",
        "Let $N_{\\text{epochs}}$ be the number of epochs and $\\eta$ be the learning rate. \n",
        "We get the following training algorithm:\n",
        "\n",
        "\n",
        "<center><img width=\"500\" src = \"https://drive.google.com/uc?export=view&id=1Od3xCvMWKOBhMpccmKOtoY3aT3UTJB5Z\"></center>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q13:</font>\n",
        "<br><font color='green'>\n",
        "Implement the gradient descent training algorithm (Algorithm 4).\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "85Ih7k7OD-py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize weights\n",
        "W = np.random.randn(V, D) / np.sqrt(V + D)\n",
        "b = np.zeros(V)\n",
        "W_tilde = np.random.randn(V,D) / np.sqrt(V + D)\n",
        "b_tilde = np.zeros(V)\n",
        "\n",
        "\n",
        "costs = []\n",
        "for epoch in range(epochs):\n",
        "    # epsilon (V, V) matrix such that epsilon_{ij} = logX_{ij} - W_i^T W_tilde_j - b_i - b_tilde_j\n",
        "    epsilon = logX - W.dot(W_tilde.T) - b.reshape(V, 1) - b_tilde.reshape(1, V)\n",
        "    # cost function sum_{ij} fX_{ij} (logX_{ij} - W_i^T W_tilde_j - b_i - b_tilde_j)^2 = sum_{ij} fX_{ij} epsilon_{ij}^2\n",
        "    cost = (fX * epsilon * epsilon).sum()\n",
        "    costs.append(cost)\n",
        "    print(\"epoch: {}...cost: {}\".format(epoch, cost))\n",
        "\n",
        "    # Update W\n",
        "    print(\"Update W..\")\n",
        "    for i in range(V):\n",
        "        if i%1000==0:\n",
        "            print(\"Epoch {}... W is updated for {} words out of {}\".format(epoch, i, V))\n",
        "        # W_i -= learning_rate*(-2*sum_{j'} fX_{i,j'}*epsilon_{i,j'}*W_tilde_j')\n",
        "        W[i] -= -2*learning_rate*(fX[i,:]*epsilon[i,:]).dot(W_tilde)\n",
        "\n",
        "\n",
        "    # update b\n",
        "    for i in range(V):\n",
        "        if i%1000==0:\n",
        "            print(\"Epoch {}... b is updated for {} words out of {}\".format(epoch, i, V))\n",
        "        # b_i -= learning_rate*(-2*sum_{j'} fX_{i,j'}*epsilon_{i,j'}\n",
        "        b[i] -= -2*learning_rate*fX[i,:].dot(epsilon[i,:])\n",
        "\n",
        "\n",
        "    # update W_tilde\n",
        "    print(\"Update W_tilde..\")\n",
        "    for j in range(V):\n",
        "        if j%1000==0:\n",
        "            print(\"Epoch {}... W_tilde is updated for {} words out of {}\".format(epoch, j, V))\n",
        "        # W_tilde_j -= learning_rate*(-2*sum_{i'} fX_{i',j}*epsilon_{i',j}*W_i')\n",
        "        W_tilde[j] -= -2*learning_rate*(fX[:,j]*epsilon[:,j]).dot(W)\n",
        "\n",
        "\n",
        "    # update b_tilde\n",
        "    print(\"Update b_tilde..\")\n",
        "    for j in range(V):\n",
        "        if j%1000==0:\n",
        "            print(\"Epoch {}... b_tilde is updated for {} words out of {}\".format(epoch, j, V))\n",
        "        # b_tilde_j -= learning_rate*(-2*sum_{i'} fX_{i',j}*epsilon_{i',j})\n",
        "        b_tilde[j] -= -2*learning_rate*fX[:,j].dot(epsilon[:,j])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xY4e1IP3mKL4",
        "outputId": "e5371ab8-9164-48a5-cf42-9a7f1c6fa07e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0...cost: 436199.0745952835\n",
            "Update W..\n",
            "Epoch 0... W is updated for 0 words out of 1000\n",
            "Epoch 0... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 0... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 0... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 1...cost: 344879.48189436167\n",
            "Update W..\n",
            "Epoch 1... W is updated for 0 words out of 1000\n",
            "Epoch 1... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 1... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 1... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 2...cost: 278229.9505702876\n",
            "Update W..\n",
            "Epoch 2... W is updated for 0 words out of 1000\n",
            "Epoch 2... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 2... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 2... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 3...cost: 228949.25595840148\n",
            "Update W..\n",
            "Epoch 3... W is updated for 0 words out of 1000\n",
            "Epoch 3... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 3... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 3... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 4...cost: 192016.24271032852\n",
            "Update W..\n",
            "Epoch 4... W is updated for 0 words out of 1000\n",
            "Epoch 4... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 4... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 4... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 5...cost: 163950.54399872836\n",
            "Update W..\n",
            "Epoch 5... W is updated for 0 words out of 1000\n",
            "Epoch 5... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 5... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 5... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 6...cost: 142319.82651926158\n",
            "Update W..\n",
            "Epoch 6... W is updated for 0 words out of 1000\n",
            "Epoch 6... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 6... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 6... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 7...cost: 125409.58581053028\n",
            "Update W..\n",
            "Epoch 7... W is updated for 0 words out of 1000\n",
            "Epoch 7... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 7... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 7... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 8...cost: 112000.58427947345\n",
            "Update W..\n",
            "Epoch 8... W is updated for 0 words out of 1000\n",
            "Epoch 8... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 8... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 8... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 9...cost: 101217.90823174697\n",
            "Update W..\n",
            "Epoch 9... W is updated for 0 words out of 1000\n",
            "Epoch 9... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 9... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 9... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 10...cost: 92427.92097690361\n",
            "Update W..\n",
            "Epoch 10... W is updated for 0 words out of 1000\n",
            "Epoch 10... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 10... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 10... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 11...cost: 85167.42512437177\n",
            "Update W..\n",
            "Epoch 11... W is updated for 0 words out of 1000\n",
            "Epoch 11... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 11... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 11... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 12...cost: 79094.61408467776\n",
            "Update W..\n",
            "Epoch 12... W is updated for 0 words out of 1000\n",
            "Epoch 12... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 12... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 12... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 13...cost: 73954.8571265648\n",
            "Update W..\n",
            "Epoch 13... W is updated for 0 words out of 1000\n",
            "Epoch 13... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 13... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 13... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 14...cost: 69556.65004844678\n",
            "Update W..\n",
            "Epoch 14... W is updated for 0 words out of 1000\n",
            "Epoch 14... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 14... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 14... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 15...cost: 65754.58080982124\n",
            "Update W..\n",
            "Epoch 15... W is updated for 0 words out of 1000\n",
            "Epoch 15... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 15... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 15... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 16...cost: 62437.17057518569\n",
            "Update W..\n",
            "Epoch 16... W is updated for 0 words out of 1000\n",
            "Epoch 16... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 16... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 16... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 17...cost: 59518.12789491194\n",
            "Update W..\n",
            "Epoch 17... W is updated for 0 words out of 1000\n",
            "Epoch 17... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 17... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 17... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 18...cost: 56930.00992563013\n",
            "Update W..\n",
            "Epoch 18... W is updated for 0 words out of 1000\n",
            "Epoch 18... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 18... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 18... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 19...cost: 54619.59367095094\n",
            "Update W..\n",
            "Epoch 19... W is updated for 0 words out of 1000\n",
            "Epoch 19... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 19... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 19... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 20...cost: 52544.47094330247\n",
            "Update W..\n",
            "Epoch 20... W is updated for 0 words out of 1000\n",
            "Epoch 20... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 20... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 20... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 21...cost: 50670.525346304355\n",
            "Update W..\n",
            "Epoch 21... W is updated for 0 words out of 1000\n",
            "Epoch 21... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 21... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 21... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 22...cost: 48970.049470352336\n",
            "Update W..\n",
            "Epoch 22... W is updated for 0 words out of 1000\n",
            "Epoch 22... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 22... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 22... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 23...cost: 47420.329977893234\n",
            "Update W..\n",
            "Epoch 23... W is updated for 0 words out of 1000\n",
            "Epoch 23... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 23... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 23... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 24...cost: 46002.57692218139\n",
            "Update W..\n",
            "Epoch 24... W is updated for 0 words out of 1000\n",
            "Epoch 24... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 24... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 24... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 25...cost: 44701.10796722748\n",
            "Update W..\n",
            "Epoch 25... W is updated for 0 words out of 1000\n",
            "Epoch 25... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 25... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 25... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 26...cost: 43502.72255091772\n",
            "Update W..\n",
            "Epoch 26... W is updated for 0 words out of 1000\n",
            "Epoch 26... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 26... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 26... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 27...cost: 42396.218458748896\n",
            "Update W..\n",
            "Epoch 27... W is updated for 0 words out of 1000\n",
            "Epoch 27... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 27... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 27... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 28...cost: 41372.015815016945\n",
            "Update W..\n",
            "Epoch 28... W is updated for 0 words out of 1000\n",
            "Epoch 28... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 28... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 28... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 29...cost: 40421.86257882913\n",
            "Update W..\n",
            "Epoch 29... W is updated for 0 words out of 1000\n",
            "Epoch 29... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 29... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 29... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 30...cost: 39538.60224811592\n",
            "Update W..\n",
            "Epoch 30... W is updated for 0 words out of 1000\n",
            "Epoch 30... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 30... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 30... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 31...cost: 38715.989322982525\n",
            "Update W..\n",
            "Epoch 31... W is updated for 0 words out of 1000\n",
            "Epoch 31... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 31... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 31... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 32...cost: 37948.54165233922\n",
            "Update W..\n",
            "Epoch 32... W is updated for 0 words out of 1000\n",
            "Epoch 32... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 32... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 32... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 33...cost: 37231.421434394004\n",
            "Update W..\n",
            "Epoch 33... W is updated for 0 words out of 1000\n",
            "Epoch 33... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 33... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 33... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 34...cost: 36560.33861225526\n",
            "Update W..\n",
            "Epoch 34... W is updated for 0 words out of 1000\n",
            "Epoch 34... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 34... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 34... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 35...cost: 35931.47188041377\n",
            "Update W..\n",
            "Epoch 35... W is updated for 0 words out of 1000\n",
            "Epoch 35... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 35... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 35... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 36...cost: 35341.403626373925\n",
            "Update W..\n",
            "Epoch 36... W is updated for 0 words out of 1000\n",
            "Epoch 36... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 36... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 36... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 37...cost: 34787.065968878545\n",
            "Update W..\n",
            "Epoch 37... W is updated for 0 words out of 1000\n",
            "Epoch 37... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 37... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 37... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 38...cost: 34265.695689278924\n",
            "Update W..\n",
            "Epoch 38... W is updated for 0 words out of 1000\n",
            "Epoch 38... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 38... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 38... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 39...cost: 33774.796336559\n",
            "Update W..\n",
            "Epoch 39... W is updated for 0 words out of 1000\n",
            "Epoch 39... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 39... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 39... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 40...cost: 33312.10615694555\n",
            "Update W..\n",
            "Epoch 40... W is updated for 0 words out of 1000\n",
            "Epoch 40... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 40... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 40... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 41...cost: 32875.57078381537\n",
            "Update W..\n",
            "Epoch 41... W is updated for 0 words out of 1000\n",
            "Epoch 41... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 41... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 41... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 42...cost: 32463.319843542802\n",
            "Update W..\n",
            "Epoch 42... W is updated for 0 words out of 1000\n",
            "Epoch 42... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 42... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 42... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 43...cost: 32073.646803554868\n",
            "Update W..\n",
            "Epoch 43... W is updated for 0 words out of 1000\n",
            "Epoch 43... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 43... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 43... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 44...cost: 31704.991521857024\n",
            "Update W..\n",
            "Epoch 44... W is updated for 0 words out of 1000\n",
            "Epoch 44... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 44... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 44... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 45...cost: 31355.925061454353\n",
            "Update W..\n",
            "Epoch 45... W is updated for 0 words out of 1000\n",
            "Epoch 45... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 45... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 45... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 46...cost: 31025.13641507494\n",
            "Update W..\n",
            "Epoch 46... W is updated for 0 words out of 1000\n",
            "Epoch 46... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 46... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 46... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 47...cost: 30711.420850453535\n",
            "Update W..\n",
            "Epoch 47... W is updated for 0 words out of 1000\n",
            "Epoch 47... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 47... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 47... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 48...cost: 30413.669638001556\n",
            "Update W..\n",
            "Epoch 48... W is updated for 0 words out of 1000\n",
            "Epoch 48... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 48... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 48... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 49...cost: 30130.860963911095\n",
            "Update W..\n",
            "Epoch 49... W is updated for 0 words out of 1000\n",
            "Epoch 49... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 49... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 49... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 50...cost: 29862.051864870744\n",
            "Update W..\n",
            "Epoch 50... W is updated for 0 words out of 1000\n",
            "Epoch 50... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 50... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 50... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 51...cost: 29606.371047345037\n",
            "Update W..\n",
            "Epoch 51... W is updated for 0 words out of 1000\n",
            "Epoch 51... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 51... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 51... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 52...cost: 29363.012476125325\n",
            "Update W..\n",
            "Epoch 52... W is updated for 0 words out of 1000\n",
            "Epoch 52... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 52... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 52... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 53...cost: 29131.229634638887\n",
            "Update W..\n",
            "Epoch 53... W is updated for 0 words out of 1000\n",
            "Epoch 53... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 53... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 53... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 54...cost: 28910.330374110825\n",
            "Update W..\n",
            "Epoch 54... W is updated for 0 words out of 1000\n",
            "Epoch 54... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 54... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 54... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 55...cost: 28699.67228074387\n",
            "Update W..\n",
            "Epoch 55... W is updated for 0 words out of 1000\n",
            "Epoch 55... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 55... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 55... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 56...cost: 28498.65850010871\n",
            "Update W..\n",
            "Epoch 56... W is updated for 0 words out of 1000\n",
            "Epoch 56... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 56... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 56... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 57...cost: 28306.733966311884\n",
            "Update W..\n",
            "Epoch 57... W is updated for 0 words out of 1000\n",
            "Epoch 57... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 57... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 57... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 58...cost: 28123.381990539543\n",
            "Update W..\n",
            "Epoch 58... W is updated for 0 words out of 1000\n",
            "Epoch 58... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 58... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 58... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 59...cost: 27948.12116950922\n",
            "Update W..\n",
            "Epoch 59... W is updated for 0 words out of 1000\n",
            "Epoch 59... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 59... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 59... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 60...cost: 27780.502579390606\n",
            "Update W..\n",
            "Epoch 60... W is updated for 0 words out of 1000\n",
            "Epoch 60... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 60... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 60... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 61...cost: 27620.107225043448\n",
            "Update W..\n",
            "Epoch 61... W is updated for 0 words out of 1000\n",
            "Epoch 61... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 61... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 61... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 62...cost: 27466.54371808642\n",
            "Update W..\n",
            "Epoch 62... W is updated for 0 words out of 1000\n",
            "Epoch 62... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 62... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 62... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 63...cost: 27319.446160462692\n",
            "Update W..\n",
            "Epoch 63... W is updated for 0 words out of 1000\n",
            "Epoch 63... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 63... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 63... b_tilde is updated for 0 words out of 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q14:</font>\n",
        "<br><font color='green'>\n",
        " Plot the list of losses at the end of each iteration in Algorithm 4.\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "6aS7uO42D_Ya"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VRIWLWrmfnR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "outputId": "5b6032bf-8ffa-441d-8092-1c68a8a56375"
      },
      "source": [
        "# Plot the costs for each epoch\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "plt.plot(costs)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"cost\")\n",
        "plt.title(\"Cost function using gradient descent\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd9PcTFwkFwo"
      },
      "source": [
        "# 3. Exercise: Introducing regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWkYPM1dkF1u"
      },
      "source": [
        "Let us introduce a regularization penalty term in the cost function. The new cost function is defined as follows:\n",
        "\n",
        "\n",
        "\\begin{equation*}\n",
        "\\tilde{J} = \\sum\\limits_{i=1}^V \\sum\\limits_{j=1}^V f(X_{ij}) (\\log X_{ij} - W_i^T \\tilde{W}_j - b_i - \\tilde{b}_j)^2 + \\lambda \\left( ||W||_{\\text{F}}^2 +   ||\\tilde{W}||_{\\text{F}}^2 + ||b||_2^2 + ||\\tilde{b} ||_2^2 \\right) \n",
        "\\end{equation*}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q15:</font>\n",
        "<br><font color='green'>\n",
        "Show that: \n",
        "\\begin{equation}\n",
        "||W||_{\\text{F}}^2 = \\sum\\limits_{i=1}^V W_i^T W_i\n",
        "\\end{equation}\n",
        "\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0Fgcb-ASEAad"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT6sJx8wTDaE"
      },
      "source": [
        "---\n",
        "**Solution:**\n",
        "\n",
        "Let $W = [w_{ij}]_{ij}$. \n",
        "\n",
        "For all $v \\in \\{1, \\dots, V \\} \\quad W_v = [w_{v1}, \\dots, w_{vD}]^T \\in \\mathcal{M}_{D, 1}(\\mathbb{R})$. \n",
        "\n",
        "Therefore:\n",
        "\n",
        "\\begin{align}\n",
        "||W||_{\\text{F}}^2 &= \\sum\\limits_{v=1}^{V} \\left( \\sum\\limits_{d=1}^{D} |w_{vd}|^2 \\right) \\\\\n",
        "&=  \\sum\\limits_{v=1}^{V} W_v^T W_v\n",
        "\\end{align}\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q16:</font>\n",
        "<br><font color='green'>\n",
        "Deduce that for all $i \\in \\{1, \\dots, V \\}$:\n",
        "\\begin{align} \n",
        "& \\nabla_{W_i} (||W||_{\\text{F}}^2) = 2W_i \\quad \\text{(3.1)} \\\\ \n",
        "&(\\text{Hint}: \\forall z \\in \\mathbb{R}^D \\ \\forall A \\in \\mathcal{M}_{D, D}(\\mathbb{R}) \\quad \\nabla_z (z^T A z) = (A + A^T)z )\n",
        "\\end{align} \n",
        "\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "JgdEJEcFEBCh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Wcp4if-VP9y"
      },
      "source": [
        "---\n",
        "**Solution:**\n",
        "\n",
        "For all $i \\in \\{1, \\dots, V \\}$:\n",
        "\n",
        "\n",
        "\\begin{align} \n",
        " \\nabla_{W_i} (||W||_{\\text{F}}^2) &= \\nabla_{W_i} \\left( \\sum\\limits_{v=1}^{V} W_v^T W_v \\right) \\\\\n",
        " &=  \\sum\\limits_{v=1}^{V} \\nabla_{W_i} \\left( W_v^T W_v \\right) \\\\\n",
        " &=  \\nabla_{W_i} \\left( W_i^T I_D W_i \\right) \\\\\n",
        " &=  (I_D + I_D^T)W_i \\\\\n",
        " &= 2 W_i\n",
        "\\end{align} \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q17:</font>\n",
        "<br><font color='green'>\n",
        "From the equations(2.1), (2.2), (2.3), (2.4) and (3.1), show that the update equations for the method of alternating least squares become: \n",
        "\n",
        "\\begin{align*}\n",
        "&W_i^{(t+1)} \\longleftarrow \\left( \\sum_{j'=1}^V f(X_{ij'}) \\tilde{W}_{j'}^{(t)} \\tilde{W}_{j'}^{(t)^T} + \\lambda I_D \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - b_i^{(t)} - \\tilde{b}_{j'}^{(t)}) \\tilde{W}_{j'}^{(t)} \\right)  \\\\\n",
        "&\\tilde{W}_j^{(t+1)} \\longleftarrow \\left( \\sum_{i'=1}^V f(X_{i' j}) W_{i'}^{(t)} W_{i'}^{(t)^T} + \\lambda I_D \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - b_{i'}^{(t)} - \\tilde{b}_{j}^{(t)}) W_{i'}^{(t)} \\right) \\\\\n",
        "&b_i^{(t+1)} \\longleftarrow \\left( \\sum_{j'=1}^V f(X_{ij'}) + \\lambda  \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - W_i^{(t)^T} \\tilde{W}_{j'}^{(t)} - \\tilde{b}_{j'}^{(t)}) \\right)  \\\\\n",
        "&\\tilde{b}_j^{(t+1)} \\longleftarrow \\left( \\sum_{i'=1}^V f(X_{i' j}) + \\lambda  \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - W_{i'}^{(t)^T} \\tilde{W}_{j}^{(t)} - b_{i'}^{(t)}) \\right) \n",
        "\\end{align*}\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "HcfbBXP2EBpp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTCLv_fKcEej"
      },
      "source": [
        "---\n",
        "**Solution:**\n",
        "\n",
        "\n",
        "Let us compute the gradients of the cost function $\\tilde{J}$ with respect to all the parameters:\n",
        "\n",
        "\n",
        "For all $i \\in \\{1, \\dots, V \\}$ and all $j \\in \\{ 1, \\dots, V \\}$:\n",
        "\n",
        "\n",
        "\\begin{align} \n",
        "& \\nabla_{W_i} \\tilde{J}(W_i) = -2 \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - b_i - \\tilde{b}_{j'} \\right) \\tilde{W}_{j'} + 2 \\lambda W_i \\quad \\text{(From (2.1) and Question 18)} \\\\\n",
        "& \\nabla_{\\tilde{W}_j} \\tilde{J}(W_j) = -2 \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'} - \\tilde{b}_j \\right) W_{i'} + 2 \\lambda \\tilde{W}_j \\quad \\text{(From (2.2) and Question 18)}  \\\\\n",
        "&\\nabla_{b_i} \\tilde{J}(b_i) = -2 \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - b_i - \\tilde{b}_{j'} \\right) + 2 \\lambda b_i \\quad \\text{(From (2.3)} \\\\\n",
        "& \\nabla_{\\tilde{b}_j} \\tilde{J}(\\tilde{b}_j) = -2 \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'} - \\tilde{b}_j \\right) + 2 \\lambda \\tilde{b}_j \\quad \\text{(From (2.4)} \n",
        "\\end{align}\n",
        "\n",
        "Let us deduce the update equations:\n",
        "\n",
        "* For $W_i$:\n",
        "\\begin{align*}\n",
        "\\nabla_{W_i} J(W_i) = 0 & \\iff -2 \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - b_i - \\tilde{b}_{j'} \\right) \\tilde{W}_{j'} + 2 \\lambda W_i = 0  \\\\\n",
        "& \\iff \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - b_i - \\tilde{b}_{j'} \\right) \\tilde{W}_{j'} = \\sum_{j'=1}^V f(X_{ij'})  W_i^T \\tilde{W}_{j'} \\tilde{W}_{j'} + \\lambda W_i \\\\\n",
        "& \\iff \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - b_i - \\tilde{b}_{j'} \\right) \\tilde{W}_{j'} = \\left( \\sum_{j'=1}^V f(X_{ij'})  \\tilde{W}_{j'}^T \\tilde{W}_{j'} + \\lambda I_D \\right) W_i   \\\\\n",
        "& \\iff W_i = \\left( \\sum_{j'=1}^V f(X_{ij'}) \\tilde{W}_{j'} \\tilde{W}_{j'}^T + \\lambda I_D \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - b_i - \\tilde{b}_{j'}) \\tilde{W}_{j'} \\right) \n",
        "\\end{align*}\n",
        "\n",
        "* For $\\tilde{W}_{j}$:\n",
        "\n",
        "\\begin{align*}\n",
        "\\nabla_{\\tilde{W}_j} J(\\tilde{W}_j) = 0 & \\iff  -2 \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'} - \\tilde{b}_j \\right) + 2 \\lambda \\tilde{W}_j = 0   \\\\\n",
        "&\\iff \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - b_{i'} - \\tilde{b}_j \\right) W_{i'} = \\sum_{i'=1}^V f(X_{i' j})  W_{i'}^T \\tilde{W}_j  W_{i'} + \\lambda \\tilde{W}_j \\\\\n",
        "& \\iff \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - b_{i'} - \\tilde{b}_j \\right) W_{i'} = \\left( \\sum_{i'=1}^V f(X_{i' j})  W_{i'} W_{i'}^T + \\lambda I_D \\right)  \\tilde{W}_j    \\\\\n",
        "& \\iff\\tilde{W}_j = \\left( \\sum_{i'=1}^V f(X_{i' j}) W_{i'} W_{i'}^T + \\lambda I_D \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - b_{i'} - \\tilde{b}_{j}) W_{i'} \\right)\n",
        "\\end{align*}\n",
        "\n",
        "* For $b_i$:\n",
        "\\begin{align*}\n",
        "\\nabla_{b_i} J(b_i) = 0  & \\iff -2 \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - b_i - \\tilde{b}_{j'} \\right) + 2 \\lambda b_i = 0 \\\\\n",
        "& \\iff \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - \\tilde{b}_{j'} \\right)  = \\left( \\sum_{j'=1}^V f(X_{ij'}) + \\lambda \\right) b_i \\\\\n",
        "& \\iff b_i = \\left( \\sum_{j'=1}^V f(X_{ij'}) + \\lambda \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - \\tilde{b}_{j'}) \\right)\n",
        "\\end{align*}\n",
        "\n",
        "* For $\\tilde{b}_j$:\n",
        "\\begin{align*}\n",
        "\\nabla_{\\tilde{b}_j} J(\\tilde{b}_j) = 0  & \\iff -2 \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'} - \\tilde{b}_j \\right) + 2 \\lambda \\tilde{b}_j  = 0  \\\\\n",
        "& \\iff \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'} -  \\right) = \\left(\\sum_{i'=1}^V f(X_{i' j}) + \\lambda \\right) \\tilde{b}_j \\\\\n",
        "& \\iff \\tilde{b}_j = \\left( \\sum_{i'=1}^V f(X_{i' j}) + \\lambda \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - W_{i'}^T \\tilde{W}_{j} - b_{i'}) \\right) \n",
        "\\end{align*}\n",
        "\n",
        "We deduce the following update equations: \n",
        "\n",
        "\\begin{align*}\n",
        "&W_i^{(t+1)} \\longleftarrow \\left( \\sum_{j'=1}^V f(X_{ij'}) \\tilde{W}_{j'}^{(t)} \\tilde{W}_{j'}^{(t)^T} + \\lambda I_D \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - b_i^{(t)} - \\tilde{b}_{j'}^{(t)}) \\tilde{W}_{j'}^{(t)} \\right)  \\\\\n",
        "&\\tilde{W}_j^{(t+1)} \\longleftarrow \\left( \\sum_{i'=1}^V f(X_{i' j}) W_{i'}^{(t)} W_{i'}^{(t)^T} + \\lambda I_D \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - b_{i'}^{(t)} - \\tilde{b}_{j}^{(t)}) W_{i'}^{(t)} \\right) \\\\\n",
        "&b_i^{(t+1)} \\longleftarrow \\left( \\sum_{j'=1}^V f(X_{ij'}) + \\lambda  \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - W_i^{(t)^T} \\tilde{W}_{j'}^{(t)} - \\tilde{b}_{j'}^{(t)}) \\right)  \\\\\n",
        "&\\tilde{b}_j^{(t+1)} \\longleftarrow \\left( \\sum_{i'=1}^V f(X_{i' j}) + \\lambda  \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - W_{i'}^{(t)^T} \\tilde{W}_{j}^{(t)} - b_{i'}^{(t)}) \\right) \n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q18:</font>\n",
        "<br><font color='green'>\n",
        " What would be the update equations for minimizing the new loss function $\\tilde{J}$ by using the gradient descent algorithm.\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "OcsjfGD9EC2C"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-7Z2AL-hU2w"
      },
      "source": [
        "---\n",
        "**Solution:**\n",
        "\n",
        "We will set a number of epochs $N_{\\text{epochs}}$ and a learning rate $\\eta$. \n",
        "\n",
        "* Initialize randomly $W^{(0)}, \\tilde{W}^{(0)}, b^{(0)}, \\tilde{b}^{(0)}$\n",
        "* For $t \\in \\{0, \\dots, N_{\\text{epochs}}-2\\}$:\n",
        "  * For $i \\in \\{0, \\dots, V-1 \\}$:\n",
        "  \\begin{align}\n",
        "  &W_i^{(t+1)} \\longleftarrow W_i^{(t)} - \\eta \\nabla_{W_i} \\tilde{J} (W_i^{(t)}) \\\\\n",
        "  &\\text{with} \\quad \\nabla_{W_i} \\tilde{J} (W_i) = -2 \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - b_i - \\tilde{b}_{j'} \\right) \\tilde{W}_{j'} + 2 \\lambda W_i\n",
        "  \\end{align}\n",
        "\n",
        "  * For $j \\in \\{0, \\dots, V-1 \\}$:\n",
        "  \\begin{align}\n",
        "  &\\tilde{W}_j^{(t+1)} \\longleftarrow \\tilde{W}_j^{(t)} - \\eta \\nabla_{\\tilde{W}_j} \\tilde{J} (\\tilde{W}_j^{(t)}) \\\\\n",
        "  &\\text{with} \\quad \\nabla_{\\tilde{W}_j} \\tilde{J} (\\tilde{W}_j) = -2 \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'} - \\tilde{b}_j \\right) + 2 \\lambda \\tilde{W}_j \n",
        "  \\end{align}\n",
        "\n",
        "  * For $i \\in \\{0, \\dots, V-1 \\}$:\n",
        "  \\begin{align}\n",
        "  &b_i^{(t+1)} \\longleftarrow b_i^{(t)} - \\eta \\nabla_{b_i} \\tilde{J} (b_i^{(t)}) \\\\\n",
        "  &\\text{with} \\quad \\nabla_{b_i} \\tilde{J} (b_i) = -2 \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - b_i - \\tilde{b}_{j'} \\right) + 2 \\lambda b_i\n",
        "  \\end{align}\n",
        "\n",
        "  * For $j \\in \\{0, \\dots, V-1 \\}$:\n",
        "  \\begin{align}\n",
        "  &\\tilde{b}_j^{(t+1)} \\longleftarrow \\tilde{b}_j^{(t)} - \\eta \\nabla_{\\tilde{b}_j} \\tilde{J} (\\tilde{b}_j^{(t)}) \\\\\n",
        "  &\\text{with} \\quad \\nabla_{\\tilde{b}_j} \\tilde{J} (\\tilde{b}_j) = -2 \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'} - \\tilde{b}_j \\right) + 2 \\lambda \\tilde{b}_j \n",
        "  \\end{align}\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix "
      ],
      "metadata": {
        "id": "I8J7ErjCVRJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<br><font color='green'>\n",
        "Let us show that:\n",
        "\n",
        "\\begin{align*}\n",
        "&\\nabla_{W_i} J(W_i) = 0 \\iff W_i = \\left( \\sum_{j'=1}^V f(X_{ij'}) \\tilde{W}_{j'} \\tilde{W}_{j'}^T \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - b_i - \\tilde{b}_{j'}) \\tilde{W}_{j'} \\right)  \\\\\n",
        "&\\nabla_{\\tilde{W}_j} J(\\tilde{W}_j) = 0 \\iff \\tilde{W}_j = \\left( \\sum_{i'=1}^V f(X_{i' j}) W_{i'} W_{i'}^T \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - b_{i'} - \\tilde{b}_{j}) W_{i'} \\right)  \\\\\n",
        "&\\nabla_{b_i} J(b_i) = 0 \\iff b_i = \\left( \\sum_{j'=1}^V f(X_{ij'})  \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - \\tilde{b}_{j'}) \\right)  \\\\\n",
        "&\\nabla_{\\tilde{b}_j} J(\\tilde{b}_j) = 0 \\iff \\tilde{b}_j = \\left( \\sum_{i'=1}^V f(X_{i' j})  \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - W_{i'}^T \\tilde{W}_{j} - b_{i'}) \\right) \n",
        "\\end{align*}\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ZxPxjvefVel3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--- \n",
        "**Proof:**\n",
        "\n",
        "First, we need to prove a preliminary result:\n",
        "\n",
        "\\begin{equation*}\n",
        "  \\forall a,b \\in \\mathcal{M}_{D, 1}(\\mathbb{R}) \\quad (a^T b) \\ b = (b \\ b^T) \\ a \\quad (\\Delta)\n",
        "\\end{equation*}\n",
        "\n",
        "\n",
        "Indeed, \n",
        "\n",
        "\\begin{align}\n",
        "\\forall a,b \\in \\mathcal{M}_{D, 1}(\\mathbb{R}) \\quad (a^T b) \\ b  &= b \\ (a^T b) \\\\\n",
        "&= b \\ (b^T a) \\quad (\\text{As} \\ a^Tb \\ \\text{is a scalar, it's equal to its transpose}) \\\\\n",
        "&=  (b b^T) \\ a \n",
        "\\end{align}\n",
        "\n",
        "For all $i \\in \\{1, \\dots, V \\}$ and for all $j \\in \\{1, \\dots, V \\}$. \n",
        "\n",
        "Let us find the optimal parameters by setting the gradients to 0:\n",
        "\n",
        "* For $W_i$:\n",
        "\\begin{align*}\n",
        "\\nabla_{W_i} J(W_i) = 0 & \\iff -2 \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - b_i - \\tilde{b}_{j'} \\right) \\tilde{W}_{j'} = 0 \\quad \\text{(From (2.1))} \\\\\n",
        "& \\iff \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - b_i - \\tilde{b}_{j'} \\right) \\tilde{W}_{j'} = \\sum_{j'=1}^V f(X_{ij'})  W_i^T \\tilde{W}_{j'} \\tilde{W}_{j'} \\\\\n",
        "& \\iff \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - b_i - \\tilde{b}_{j'} \\right) \\tilde{W}_{j'} = \\left( \\sum_{j'=1}^V f(X_{ij'})  \\tilde{W}_{j'} \\tilde{W}_{j'}^T \\right) W_i  \\quad (\\text{From} \\ (\\Delta)) \\\\\n",
        "& \\iff W_i = \\left( \\sum_{j'=1}^V f(X_{ij'}) \\tilde{W}_{j'} \\tilde{W}_{j'}^T \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - b_i - \\tilde{b}_{j'}) \\tilde{W}_{j'} \\right) \n",
        "\\end{align*}\n",
        "\n",
        "* For $\\tilde{W}_{j}$:\n",
        "\n",
        "\\begin{align*}\n",
        "\\nabla_{\\tilde{W}_j} J(\\tilde{W}_j) = 0 & \\iff  -2 \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'} - \\tilde{b}_j \\right) = 0  \\quad \\text{(From (2.2))} \\\\\n",
        "&\\iff \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - b_{i'} - \\tilde{b}_j \\right) W_{i'} = \\sum_{i'=1}^V f(X_{i' j})  W_{i'}^T \\tilde{W}_j  W_{i'} \\\\\n",
        "& \\iff \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - b_{i'} - \\tilde{b}_j \\right) W_{i'} = \\left( \\sum_{i'=1}^V f(X_{i' j})  W_{i'} W_{i'}^T \\right)  \\tilde{W}_j    \\\\\n",
        "& \\iff\\tilde{W}_j = \\left( \\sum_{i'=1}^V f(X_{i' j}) W_{i'} W_{i'}^T \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - b_{i'} - \\tilde{b}_{j}) W_{i'} \\right)\n",
        "\\end{align*}\n",
        "\n",
        "* For $b_i$:\n",
        "\\begin{align*}\n",
        "\\nabla_{b_i} J(b_i) = 0  & \\iff -2 \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - b_i - \\tilde{b}_{j'} \\right) = 0 \\quad \\text{(From (2.3))} \\\\\n",
        "& \\iff \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - \\tilde{b}_{j'} \\right)  = \\left( \\sum_{j'=1}^V f(X_{ij'}) \\right) b_i \\\\\n",
        "& \\iff b_i = \\left( \\sum_{j'=1}^V f(X_{ij'})  \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - \\tilde{b}_{j'}) \\right)\n",
        "\\end{align*}\n",
        "\n",
        "* For $\\tilde{b}_j$:\n",
        "\\begin{align*}\n",
        "\\nabla_{\\tilde{b}_j} J(\\tilde{b}_j) = 0  & \\iff -2 \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'} - \\tilde{b}_j \\right) = 0 \\quad \\text{(From (2.4))} \\\\\n",
        "& \\iff \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'}   \\right) = \\left(\\sum_{i'=1}^V f(X_{i' j}) \\right) \\tilde{b}_j \\\\\n",
        "& \\iff \\tilde{b}_j = \\left( \\sum_{i'=1}^V f(X_{i' j})  \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - W_{i'}^T \\tilde{W}_{j} - b_{i'}) \\right) \n",
        "\\end{align*}\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Fs8YgSFKXIHP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_I-tM9dK3dY"
      },
      "source": [
        "### Contact\n",
        "\n",
        "If you have any question regarding this notebook, do not hesitate to contact: h.madmoun@imperial.ac.uk\n",
        "\n"
      ]
    }
  ]
}